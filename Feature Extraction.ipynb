{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9786b27",
   "metadata": {},
   "source": [
    "# Ibrahim's Custom Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q antropy statsmodels librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGFeatureExtractor updated with Time, Frequency, Time-Freq, and Decomposition domains.\n",
      "Total feature count: 108\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "max\n",
      "min\n",
      "range\n",
      "iqr\n",
      "kurtosis\n",
      "skew\n",
      "Q1\n",
      "Q3\n",
      "mean_abs_diff1_raw\n",
      "mean_abs_diff2_raw\n",
      "mean_abs_diff1_norm\n",
      "mean_abs_diff2_norm\n",
      "ar_coeff_0\n",
      "ar_coeff_1\n",
      "ar_coeff_2\n",
      "ar_coeff_3\n",
      "higuchi_fd\n",
      "svd_entropy\n",
      "perm_entropy\n",
      "app_entropy\n",
      "sample_entropy\n",
      "petrosian_fd\n",
      "katz_fd\n",
      "lziv_complexity\n",
      "DFA\n",
      "spectral_entropy\n",
      "shannon_entropy\n",
      "weighted_permutation_entropy\n",
      "fuzzy_entropy\n",
      "distribution_entropy\n",
      "hjorth_mob\n",
      "hjorth_comp\n",
      "hjorth_act\n",
      "rms\n",
      "line_length\n",
      "nonlinear_energy\n",
      "lbp_mean\n",
      "zero_crossings\n",
      "emg_envelope_mean\n",
      "aEEG_mean\n",
      "aEEG_lower_margin\n",
      "aEEG_upper_margin\n",
      "aEEG_bandwidth\n",
      "aEEG_log_mean\n",
      "aEEG_log_bandwidth\n",
      "peak_freq\n",
      "weighted_mean_freq\n",
      "median_freq\n",
      "bandwidth\n",
      "spectral_edge_freq_95\n",
      "spectral_edge_freq_90\n",
      "intensity_weighted_mean_freq\n",
      "power_delta\n",
      "power_theta\n",
      "power_alpha\n",
      "power_beta\n",
      "power_gamma\n",
      "total_power\n",
      "ratio_delta_alpha\n",
      "ratio_theta_beta\n",
      "hurst_exponent\n",
      "rel_power_delta\n",
      "rel_power_theta\n",
      "rel_power_alpha\n",
      "rel_power_beta\n",
      "rel_power_gamma\n",
      "ratio_theta_alpha\n",
      "ratio_alpha_beta\n",
      "ratio_beta_gamma\n",
      "ratio_slow_fast\n",
      "ratio_slow_alpha_beta\n",
      "ratio_alpha_total\n",
      "hilbert_amp_mean\n",
      "hilbert_freq_mean\n",
      "spectrogram_mean\n",
      "spectrogram_std\n",
      "log_variance\n",
      "mean_frequency_tf\n",
      "median_frequency_tf\n",
      "peak_frequency_timefreq_mean\n",
      "peak_frequency_timefreq_std\n",
      "dwt_A4_energy\n",
      "dwt_A4_mean\n",
      "dwt_A4_std\n",
      "dwt_A4_entropy\n",
      "dwt_D4_energy\n",
      "dwt_D4_mean\n",
      "dwt_D4_std\n",
      "dwt_D4_entropy\n",
      "dwt_D3_energy\n",
      "dwt_D3_mean\n",
      "dwt_D3_std\n",
      "dwt_D3_entropy\n",
      "dwt_D2_energy\n",
      "dwt_D2_mean\n",
      "dwt_D2_std\n",
      "dwt_D2_entropy\n",
      "dwt_D1_energy\n",
      "dwt_D1_mean\n",
      "dwt_D1_std\n",
      "dwt_D1_entropy\n",
      "cwt_max_power\n",
      "cwt_mean_power\n",
      "arma_ar1_coeff\n",
      "arma_resid_std\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "from scipy.integrate import simpson\n",
    "import antropy as ant\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pywt\n",
    "\n",
    "class EEGFeatureExtractor:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    # --- Helper: 1D Local Binary Pattern ---\n",
    "    def _lbp_1d(self, x):\n",
    "        code = np.zeros(len(x)-2)\n",
    "        for i in range(1, len(x)-1):\n",
    "            b1 = 1 if x[i-1] >= x[i] else 0\n",
    "            b2 = 1 if x[i+1] >= x[i] else 0\n",
    "            code[i-1] = b1 * 2 + b2 * 1 \n",
    "        return np.mean(code) \n",
    "    \n",
    "    # --- Helper: EMG Envelope ---\n",
    "    def _emg_envelope(self, x):\n",
    "        sos = signal.butter(4, 30, 'hp', fs=self.sfreq, output='sos')\n",
    "        filt = signal.sosfilt(sos, x)\n",
    "        rect = np.abs(filt)\n",
    "        sos_lp = signal.butter(4, 10, 'lp', fs=self.sfreq, output='sos')\n",
    "        envelope = signal.sosfilt(sos_lp, rect)\n",
    "        return np.mean(envelope)\n",
    "    \n",
    "        # --- Helper: safe Weighted Permutation Entropy ---\n",
    "    def _weighted_perm_entropy(self, x, order=3, delay=1, normalize=True):\n",
    "        \"\"\"\n",
    "        Wrapper around ant.weighted_permutation_entropy with simple safety checks.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)\n",
    "        # Need enough points for embedding\n",
    "        if len(x) < order * delay + 1:\n",
    "            return np.nan\n",
    "        # Constant signal -> zero entropy\n",
    "        if np.allclose(x, x[0]):\n",
    "            return 0.0\n",
    "        try:\n",
    "            return ant.weighted_permutation_entropy(\n",
    "                x, order=order, delay=delay, normalize=normalize\n",
    "            )\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # --- Helper: safe Fuzzy Entropy ---\n",
    "    def _fuzzy_entropy(self, x, m=2, r=None):\n",
    "        \"\"\"\n",
    "        Wrapper around ant.fuzzy_entropy with automatic r and safety checks.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)\n",
    "        if len(x) < m + 2:\n",
    "            return np.nan\n",
    "        if r is None:\n",
    "            r = 0.2 * np.std(x)  # common heuristic\n",
    "        # If std ~ 0, r will be tiny and can blow up numerics\n",
    "        if r == 0 or np.isnan(r):\n",
    "            return 0.0\n",
    "        try:\n",
    "            return ant.fuzzy_entropy(x, m=m, r=r)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # --- Helper: safe Distribution Entropy ---\n",
    "    def _distribution_entropy(self, x, m=2, tau=1, normalize=True):\n",
    "        \"\"\"\n",
    "        Wrapper around ant.distribution_entropy with safety checks.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)\n",
    "        if len(x) < m * tau + 1:\n",
    "            return np.nan\n",
    "        # Constant signals -> low/zero entropy\n",
    "        if np.allclose(x, x[0]):\n",
    "            return 0.0\n",
    "        try:\n",
    "            return ant.distribution_entropy(\n",
    "                x, m=m, tau=tau, normalize=normalize\n",
    "            )\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "        \n",
    "        # --- Helper: aEEG envelope (core processing) ---\n",
    "    def _aeeg_envelope(self, x, sf=None):\n",
    "        \"\"\"\n",
    "        Compute the aEEG envelope:\n",
    "            1. 2-15 Hz band-pass filter\n",
    "            2. Rectify\n",
    "            3. Low-pass filter (~0.5 Hz)\n",
    "\n",
    "        Returns:\n",
    "            envelope : np.ndarray or None if too short / invalid\n",
    "        \"\"\"\n",
    "        x = np.asarray(x)\n",
    "        sf = self.sfreq if sf is None else sf\n",
    "\n",
    "        # Require at least 2 seconds of data\n",
    "        if len(x) < 2 * sf:\n",
    "            return None\n",
    "\n",
    "        # 1. Band-pass 2–15 Hz\n",
    "        sos = signal.butter(4, [2, 15], btype='bandpass', fs=sf, output='sos')\n",
    "        bp = signal.sosfilt(sos, x)\n",
    "\n",
    "        # 2. Rectify\n",
    "        rect = np.abs(bp)\n",
    "\n",
    "        # 3. Smooth with low-pass (0.5 Hz ≈ 2 s time constant)\n",
    "        cutoff = 0.5\n",
    "        sos_lp = signal.butter(4, cutoff, btype='low', fs=sf, output='sos')\n",
    "        envelope = signal.sosfilt(sos_lp, rect)\n",
    "\n",
    "        return envelope\n",
    "\n",
    "    # --- Helper: aEEG feature block (clinical-style) ---\n",
    "    def _aeeg_features(self, x, sf=None):\n",
    "        \"\"\"\n",
    "        Compute a set of aEEG-derived features similar to clinical aEEG:\n",
    "            - aeeg_mean: mean envelope amplitude\n",
    "            - aeeg_lower_margin: 5th percentile of envelope\n",
    "            - aeeg_upper_margin: 95th percentile of envelope\n",
    "            - aeeg_bandwidth: upper - lower\n",
    "            - aeeg_log_mean: mean log10 amplitude\n",
    "            - aeeg_log_bandwidth: log10(upper) - log10(lower)\n",
    "\n",
    "        Returns:\n",
    "            dict of feature_name -> value (np.nan if cannot compute)\n",
    "        \"\"\"\n",
    "        sf = self.sfreq if sf is None else sf\n",
    "        env = self._aeeg_envelope(x, sf=sf)\n",
    "\n",
    "        feats = {\n",
    "            'aEEG_mean': np.nan,\n",
    "            'aEEG_lower_margin': np.nan,\n",
    "            'aEEG_upper_margin': np.nan,\n",
    "            'aEEG_bandwidth': np.nan,\n",
    "            'aEEG_log_mean': np.nan,\n",
    "            'aEEG_log_bandwidth': np.nan,\n",
    "        }\n",
    "\n",
    "        if env is None:\n",
    "            return feats\n",
    "\n",
    "        # Guard against non-positive values in log\n",
    "        eps = 1e-6\n",
    "        env_safe = env + eps\n",
    "\n",
    "        lower = np.percentile(env, 5)\n",
    "        upper = np.percentile(env, 95)\n",
    "        mean_amp = float(np.mean(env))\n",
    "\n",
    "        feats['aEEG_mean'] = mean_amp\n",
    "        feats['aEEG_lower_margin'] = float(lower)\n",
    "        feats['aEEG_upper_margin'] = float(upper)\n",
    "        feats['aEEG_bandwidth'] = float(upper - lower)\n",
    "\n",
    "        feats['aEEG_log_mean'] = float(np.mean(np.log10(env_safe)))\n",
    "        feats['aEEG_log_bandwidth'] = float(\n",
    "            np.log10(upper + eps) - np.log10(lower + eps)\n",
    "        )\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "    # --- Helper: Hurst exponent (rescaled range / R/S analysis) ---\n",
    "    def _hurst_exponent(self, x):\n",
    "        \"\"\"\n",
    "        Computes the Hurst exponent using classical R/S analysis.\n",
    "\n",
    "        Returns:\n",
    "            H (float): Hurst exponent in [0, 1] or np.nan on error.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x, dtype=float)\n",
    "\n",
    "        # Need sufficient data\n",
    "        if len(x) < 200:\n",
    "            return np.nan\n",
    "        \n",
    "        # Constant signals → H = 0.5 (pure noise-like)\n",
    "        if np.allclose(x, x[0]):\n",
    "            return 0.5\n",
    "        \n",
    "        # Segment sizes (log-spaced)\n",
    "        N = len(x)\n",
    "        segment_sizes = np.floor(np.logspace(1.3, np.log10(N / 2), num=10)).astype(int)\n",
    "        segment_sizes = np.unique(segment_sizes)\n",
    "        \n",
    "        RS_vals = []\n",
    "        sizes_used = []\n",
    "\n",
    "        for size in segment_sizes:\n",
    "            if size < 10 or size >= N:\n",
    "                continue\n",
    "\n",
    "            n_segments = N // size\n",
    "            if n_segments < 2:\n",
    "                continue\n",
    "\n",
    "            RS_per_seg = []\n",
    "\n",
    "            for i in range(n_segments):\n",
    "                seg = x[i*size:(i+1)*size]\n",
    "                mean_seg = np.mean(seg)\n",
    "                Y = np.cumsum(seg - mean_seg)\n",
    "                R = np.max(Y) - np.min(Y)\n",
    "                S = np.std(seg)\n",
    "\n",
    "                if S == 0:\n",
    "                    continue\n",
    "\n",
    "                RS_per_seg.append(R / S)\n",
    "\n",
    "            if len(RS_per_seg) > 0:\n",
    "                RS_vals.append(np.mean(RS_per_seg))\n",
    "                sizes_used.append(size)\n",
    "\n",
    "        # Need at least 2 points for regression\n",
    "        if len(RS_vals) < 2:\n",
    "            return np.nan\n",
    "\n",
    "        # Log–log linear regression\n",
    "        log_sizes = np.log10(sizes_used)\n",
    "        log_RS = np.log10(RS_vals)\n",
    "\n",
    "        slope, _ = np.polyfit(log_sizes, log_RS, 1)\n",
    "        H = float(slope)\n",
    "\n",
    "        return H\n",
    "\n",
    "\n",
    "\n",
    "    def extract_time_domain(self, x):\n",
    "        f = {}\n",
    "        # 1. Statistics\n",
    "        f['mean'] = np.mean(x)\n",
    "        f['std'] = np.std(x)\n",
    "        f['median'] = np.median(x)\n",
    "        f['max'] = np.max(x)\n",
    "        f['min'] = np.min(x)\n",
    "        f['range'] = f['max'] - f['min']\n",
    "        f['iqr'] = stats.iqr(x)\n",
    "        f['kurtosis'] = stats.kurtosis(x)\n",
    "        f['skew'] = stats.skew(x)\n",
    "        f['Q1'] = np.percentile(x, 25)\n",
    "        f['Q3'] = np.percentile(x, 75)\n",
    "        \n",
    "        # 2. Differences\n",
    "        diff1 = np.diff(x)\n",
    "        diff2 = np.diff(diff1)\n",
    "        f['mean_abs_diff1_raw'] = np.mean(np.abs(diff1))\n",
    "        f['mean_abs_diff2_raw'] = np.mean(np.abs(diff2))\n",
    "        f['mean_abs_diff1_norm'] = f['mean_abs_diff1_raw'] / (f['std'] + 1e-6)\n",
    "        f['mean_abs_diff2_norm'] = f['mean_abs_diff2_raw'] / (f['std'] + 1e-6)\n",
    "        \n",
    "        # 3. AR Coefficients (Order 4)\n",
    "        try:\n",
    "            res = AutoReg(x, lags=4, trend='n').fit()\n",
    "            for i, c in enumerate(res.params):\n",
    "                f[f'ar_coeff_{i}'] = c\n",
    "        except:\n",
    "            for i in range(4): f[f'ar_coeff_{i}'] = 0\n",
    "            \n",
    "        # 4. Nonlinear / Entropy \n",
    "        f['higuchi_fd'] = ant.higuchi_fd(x)\n",
    "        f['svd_entropy'] = ant.svd_entropy(x, normalize=True)\n",
    "        f['perm_entropy'] = ant.perm_entropy(x, normalize=True)\n",
    "        f['app_entropy'] = ant.app_entropy(x)\n",
    "        f['sample_entropy'] = ant.sample_entropy(x)\n",
    "        f['petrosian_fd'] = ant.petrosian_fd(x)\n",
    "        f['katz_fd'] = ant.katz_fd(x)\n",
    "        f['lziv_complexity'] = ant.lziv_complexity(x > np.mean(x), normalize=True)\n",
    "        f['DFA'] = ant.detrended_fluctuation(x)\n",
    "        f['shannon_entropy'] = stats.entropy(np.abs(x) + 1e-6)\n",
    "        f['weighted_permutation_entropy'] = self._weighted_perm_entropy(x, normalize=True)\n",
    "        f['fuzzy_entropy'] = self._fuzzy_entropy(x)\n",
    "        f['distribution_entropy'] = self._distribution_entropy(x, normalize=True)\n",
    "        \n",
    "        # 5. Hjorth\n",
    "        f['hjorth_mob'], f['hjorth_comp'] = ant.hjorth_params(x)\n",
    "        f['hjorth_act'] = f['std']**2\n",
    "        \n",
    "        # 6. Energy/Power\n",
    "        f['rms'] = np.sqrt(np.mean(x**2))\n",
    "        f['line_length'] = np.sum(np.abs(diff1))\n",
    "        f['nonlinear_energy'] = np.mean(x[1:-1]**2 - x[:-2] * x[2:])\n",
    "        \n",
    "        # 7. Patterns\n",
    "        f['lbp_mean'] = self._lbp_1d(x)\n",
    "        f['zero_crossings'] = ant.num_zerocross(x)\n",
    "        f['emg_envelope_mean'] = self._emg_envelope(x)\n",
    "        aeeg_feats = self._aeeg_features(x)\n",
    "        f.update(aeeg_feats)\n",
    "        \n",
    "        return f\n",
    "\n",
    "    def extract_frequency_domain(self, x):\n",
    "        f = {}\n",
    "        freqs, psd = signal.welch(x, self.sfreq, nperseg=min(len(x), 256))\n",
    "        psd_norm = psd / (np.sum(psd) + 1e-6)\n",
    "        \n",
    "        f['peak_freq'] = freqs[np.argmax(psd)]\n",
    "        f['spectral_entropy'] = ant.spectral_entropy(x, self.sfreq, method='welch', normalize=True)\n",
    "        f['weighted_mean_freq'] = np.sum(freqs * psd_norm)\n",
    "        f['median_freq'] = freqs[np.where(np.cumsum(psd_norm) >= 0.5)[0][0]]\n",
    "        f['bandwidth'] = np.sqrt(np.sum(((freqs - f['weighted_mean_freq'])**2) * psd_norm))\n",
    "        f['spectral_edge_freq_95'] = freqs[np.where(np.cumsum(psd_norm) >= 0.95)[0][0]]\n",
    "        f['spectral_edge_freq_90'] = freqs[np.where(np.cumsum(psd_norm) >= 0.90)[0][0]]\n",
    "        f['intensity_weighted_mean_freq'] = np.sum(freqs * (psd**2)) / (np.sum(psd**2) + 1e-6)\n",
    "        \n",
    "        bands = {'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), 'beta': (13, 30), 'gamma': (30, 80)}\n",
    "        total_power = 0\n",
    "        for band, (low, high) in bands.items():\n",
    "            idx = np.logical_and(freqs >= low, freqs <= high)\n",
    "            power = simpson(y = psd[idx], x = freqs[idx])\n",
    "            f[f'power_{band}'] = power\n",
    "            total_power += power\n",
    "            \n",
    "        f['total_power'] = total_power\n",
    "        f['ratio_delta_alpha'] = f['power_delta'] / (f['power_alpha'] + 1e-6)\n",
    "        f['ratio_theta_beta'] = f['power_theta'] / (f['power_beta'] + 1e-6)\n",
    "        f['hurst_exponent'] = self._hurst_exponent(x)\n",
    "        \n",
    "        f['rel_power_delta'] = f['power_delta'] / (total_power + 1e-6)\n",
    "        f['rel_power_theta'] = f['power_theta'] / (total_power + 1e-6)\n",
    "        f['rel_power_alpha'] = f['power_alpha'] / (total_power + 1e-6)\n",
    "        f['rel_power_beta'] = f['power_beta'] / (total_power +  1e-6)\n",
    "        f['rel_power_gamma'] = f['power_gamma'] / (total_power + 1e-6)\n",
    "        \n",
    "        f['ratio_theta_alpha'] = f['power_theta'] / (f['power_alpha'] + 1e-6)\n",
    "        f['ratio_alpha_beta'] = f['power_alpha'] / (f['power_beta'] + 1e-6)\n",
    "        f['ratio_beta_gamma'] = f['power_beta'] / (f['power_gamma'] + 1e-6)\n",
    "        \n",
    "        f['ratio_slow_fast'] = (f['power_delta'] + f['power_theta']) / (f['power_alpha'] + f['power_beta'] + f['power_gamma'] + 1e-6)\n",
    "        f['ratio_slow_alpha_beta'] = (f['power_delta'] + f['power_theta']) / (f['power_alpha'] + f['power_beta'] + 1e-6)\n",
    "        f['ratio_alpha_total'] = f['power_alpha'] / (total_power + 1e-6)\n",
    "        \n",
    "        # Hilbert Features\n",
    "        analytic = signal.hilbert(x)\n",
    "        amplitude = np.abs(analytic)\n",
    "        phase = np.unwrap(np.angle(analytic))\n",
    "        inst_freq = (np.diff(phase) / (2.0*np.pi) * self.sfreq)\n",
    "        f['hilbert_amp_mean'] = np.mean(amplitude)\n",
    "        f['hilbert_freq_mean'] = np.mean(inst_freq)\n",
    "        \n",
    "        return f\n",
    "\n",
    "    def extract_time_frequency_domain(self, x):\n",
    "        f = {}\n",
    "        # Short-Time Fourier Transform (STFT) / Spectrogram\n",
    "        f_stft, t_stft, Zxx = signal.stft(x, fs=self.sfreq, nperseg=min(len(x)//4, 64))\n",
    "        Sxx = np.abs(Zxx) # Magnitude spectrogram\n",
    "\n",
    "        # 1. Spectrogram Statistics\n",
    "        f['spectrogram_mean'] = np.mean(Sxx)\n",
    "        f['spectrogram_std'] = np.std(Sxx)\n",
    "        f['log_variance'] = np.var(np.log(Sxx + 1e-6))\n",
    "        \n",
    "        # 2. Frequency tracking over time\n",
    "        # Mean frequency at each time step\n",
    "        mean_freq_over_time = np.sum(Sxx * f_stft[:, None], axis=0) / (np.sum(Sxx, axis=0) + 1e-6)\n",
    "        f['mean_frequency_tf'] = np.mean(mean_freq_over_time)\n",
    "        f['median_frequency_tf'] = np.median(mean_freq_over_time)\n",
    "        \n",
    "        # 3. Peak Frequency stability\n",
    "        # Index of max power at each time step\n",
    "        peak_freq_indices = np.argmax(Sxx, axis=0)\n",
    "        peak_freqs = f_stft[peak_freq_indices]\n",
    "        f['peak_frequency_timefreq_mean'] = np.mean(peak_freqs)\n",
    "        f['peak_frequency_timefreq_std'] = np.std(peak_freqs)\n",
    "\n",
    "        return f\n",
    "\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        f = {}\n",
    "        \n",
    "        # 1. Discrete Wavelet Transform (DWT)\n",
    "        # Level 4 decomposition with Daubechies 4\n",
    "        coeffs = pywt.wavedec(x, 'db4', level=4)\n",
    "        # coeffs: [cA4, cD4, cD3, cD2, cD1]\n",
    "        coeff_names = ['A4', 'D4', 'D3', 'D2', 'D1']\n",
    "        \n",
    "        for name, c in zip(coeff_names, coeffs):\n",
    "            f[f'dwt_{name}_energy'] = np.sum(c**2)\n",
    "            f[f'dwt_{name}_mean'] = np.mean(c)\n",
    "            f[f'dwt_{name}_std'] = np.std(c)\n",
    "            # Approximate entropy of coefficients\n",
    "            f[f'dwt_{name}_entropy'] = stats.entropy(np.abs(c) + 1e-6)\n",
    "\n",
    "        # 2. Continuous Wavelet Transform (CWT) Proxy\n",
    "        # Using Ricker wavelet (Mexican Hat) for scales 1-30\n",
    "        widths = np.arange(1, 31)\n",
    "        cwtmatr, _ = pywt.cwt(x, widths, 'mexh')\n",
    "        f['cwt_max_power'] = np.max(cwtmatr**2)\n",
    "        f['cwt_mean_power'] = np.mean(cwtmatr**2)\n",
    "\n",
    "        # 3. ARMA / ARIMA Model\n",
    "        # Using a simple ARMA(1,1) to capture linear dynamics\n",
    "        # Note: Full ARIMA fitting is slow; we use conditional sum of squares or smaller iter\n",
    "        try:\n",
    "            # We fit a simple AR(1) here as proxy for ARMA to save time on 140k samples\n",
    "            # or we can try simple 1-step forecast error\n",
    "            model = AutoReg(x, lags=1, trend='c').fit()\n",
    "            f['arma_ar1_coeff'] = model.params[1]\n",
    "            f['arma_resid_std'] = np.std(model.resid)\n",
    "        except:\n",
    "            f['arma_ar1_coeff'] = 0\n",
    "            f['arma_resid_std'] = 0\n",
    "            \n",
    "        return f\n",
    "    \n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_time_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "        \n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "\n",
    "print(\"EEGFeatureExtractor updated with Time, Frequency, Time-Freq, and Decomposition domains.\")\n",
    "\n",
    "extractor = EEGFeatureExtractor()\n",
    "dummy = np.random.randn(1000)  # 4 seconds at 250Hz\n",
    "\n",
    "_ = extractor.list_feature_names(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "874a5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGFeatureExtractor class defined.\n",
      "Total feature count: 53\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "range\n",
      "skewness\n",
      "kurtosis\n",
      "rms\n",
      "max\n",
      "min\n",
      "IQR\n",
      "Q1\n",
      "Q3\n",
      "heart_rate\n",
      "heart_rate_variability\n",
      "rr_range\n",
      "qrs_count\n",
      "lpc_coeff_0\n",
      "lpc_coeff_1\n",
      "lpc_coeff_2\n",
      "lpc_coeff_3\n",
      "lpc_resid_std\n",
      "lpc_resid_var\n",
      "lpc_resid_energy\n",
      "dominant_freq\n",
      "total_power\n",
      "norm_peak_band_energy\n",
      "norm_harmonics_energy\n",
      "norm_peak_plus_harmonics_energy\n",
      "mfcc_0\n",
      "mfcc_1\n",
      "mfcc_2\n",
      "mfcc_3\n",
      "mfcc_4\n",
      "dct_mean\n",
      "dct_var\n",
      "hilbert_amp_mean\n",
      "hilbert_freq_mean\n",
      "stft_energy_mean\n",
      "stft_energy_std\n",
      "stft_peak_freq_mean\n",
      "dwt_A4_energy\n",
      "dwt_A4_std\n",
      "dwt_D4_energy\n",
      "dwt_D4_std\n",
      "dwt_D3_energy\n",
      "dwt_D3_std\n",
      "dwt_D2_energy\n",
      "dwt_D2_std\n",
      "dwt_D1_energy\n",
      "dwt_D1_std\n",
      "svd_val_0\n",
      "svd_val_1\n",
      "svd_val_2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import scipy.fftpack as fftpack\n",
    "import scipy.stats as stats\n",
    "import librosa\n",
    "import pywt\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "class ECGFeatureExtractor:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    # --- Helper: Pan-Tompkins QRS Detector (Simplified) ---\n",
    "    def _detect_qrs(self, x):\n",
    "        # A. Bandpass Filter (5-15Hz)\n",
    "        sos = signal.butter(3, [5, 15], 'bandpass', fs=self.sfreq, output='sos')\n",
    "        filtered = signal.sosfilt(sos, x)\n",
    "        \n",
    "        # B. Derivative\n",
    "        diff = np.diff(filtered)\n",
    "        \n",
    "        # C. Squaring\n",
    "        squared = diff ** 2\n",
    "        \n",
    "        # D. Moving Window Integration (150ms)\n",
    "        window_size = int(0.15 * self.sfreq)\n",
    "        integrated = np.convolve(squared, np.ones(window_size)/window_size, mode='same')\n",
    "        \n",
    "        # E. Peak Finding\n",
    "        # Height threshold relative to max signal\n",
    "        thresh = np.mean(integrated) * 2\n",
    "        peaks, _ = signal.find_peaks(integrated, height=thresh, distance=int(0.2*self.sfreq))\n",
    "        \n",
    "        return peaks\n",
    "    \n",
    "    # --- Helper: Normalized energy in peak band & harmonics ---\n",
    "    def _normalized_peak_harmonic_energy(self, freqs, psd, n_harmonics=3, band_width=0.5):\n",
    "        \"\"\"\n",
    "        Compute normalized energy around the dominant frequency and its harmonics.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        freqs : np.ndarray\n",
    "            Frequency vector from PSD (e.g., from signal.welch).\n",
    "        psd : np.ndarray\n",
    "            Power spectral density values corresponding to freqs.\n",
    "        n_harmonics : int\n",
    "            Number of harmonics to include (fundamental + this many-1).\n",
    "        band_width : float\n",
    "            Half-width (in Hz) of each band around k*f0, i.e. [k*f0 - band_width, k*f0 + band_width].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        norm_peak_energy : float\n",
    "            Normalized energy in the fundamental band.\n",
    "        norm_harmonics_energy : float\n",
    "            Normalized energy in all harmonic bands (2*f0, 3*f0, ...).\n",
    "        norm_total_peak_harmonics : float\n",
    "            Normalized energy in fundamental + harmonics together.\n",
    "        \"\"\"\n",
    "        freqs = np.asarray(freqs)\n",
    "        psd = np.asarray(psd)\n",
    "\n",
    "        # Total spectral energy\n",
    "        total_power = simpson(y = psd, x = freqs)\n",
    "        if total_power <= 0 or len(freqs) == 0:\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        # Fundamental frequency (dominant peak)\n",
    "        peak_idx = np.argmax(psd)\n",
    "        f0 = freqs[peak_idx]\n",
    "\n",
    "        energy_fund = 0.0\n",
    "        energy_harm = 0.0\n",
    "\n",
    "        for k in range(1, n_harmonics + 1):\n",
    "            fk = k * f0\n",
    "            # If the band is entirely beyond Nyquist, stop\n",
    "            if fk - band_width > freqs[-1]:\n",
    "                break\n",
    "\n",
    "            band_mask = (freqs >= fk - band_width) & (freqs <= fk + band_width)\n",
    "            if not np.any(band_mask):\n",
    "                continue\n",
    "\n",
    "            band_energy = simpson(y = psd[band_mask], x = freqs[band_mask])\n",
    "\n",
    "            if k == 1:\n",
    "                energy_fund += band_energy\n",
    "            else:\n",
    "                energy_harm += band_energy\n",
    "\n",
    "        eps = 1e-12\n",
    "        norm_peak = energy_fund / (total_power + eps)\n",
    "        norm_harm = energy_harm / (total_power + eps)\n",
    "        norm_total = (energy_fund + energy_harm) / (total_power + eps)\n",
    "\n",
    "        return float(norm_peak), float(norm_harm), float(norm_total)\n",
    "\n",
    "\n",
    "    def extract_time_domain(self, x):\n",
    "        f = {}\n",
    "        # Basic Stats\n",
    "        f['mean'] = np.mean(x)\n",
    "        f['std'] = np.std(x)\n",
    "        f['median'] = np.median(x)\n",
    "        f['range'] = np.max(x) - np.min(x)\n",
    "        f['skewness'] = stats.skew(x)\n",
    "        f['kurtosis'] = stats.kurtosis(x)\n",
    "        f['rms'] = np.sqrt(np.mean(x**2))\n",
    "        f['max'] = np.max(x)\n",
    "        f['min'] = np.min(x)\n",
    "        f['IQR'] = stats.iqr(x)\n",
    "        f['Q1'] = np.percentile(x, 25)\n",
    "        f['Q3'] = np.percentile(x, 75)\n",
    "        \n",
    "        # QRS & HR Features\n",
    "        r_peaks = self._detect_qrs(x)\n",
    "        \n",
    "        if len(r_peaks) > 1:\n",
    "            rr_intervals = np.diff(r_peaks) / self.sfreq # in seconds\n",
    "            f['heart_rate'] = 60.0 / np.mean(rr_intervals) # BPM\n",
    "            f['heart_rate_variability'] = np.std(rr_intervals) # SDNN\n",
    "            f['rr_range'] = np.max(rr_intervals) - np.min(rr_intervals)\n",
    "            f['qrs_count'] = len(r_peaks)\n",
    "        else:\n",
    "            f['heart_rate'] = 0\n",
    "            f['heart_rate_variability'] = 0\n",
    "            f['rr_range'] = 0\n",
    "            f['qrs_count'] = len(r_peaks)\n",
    "\n",
    "        # Linear Predictive Coding (LPC)\n",
    "        # Order 4 as requested\n",
    "        try:\n",
    "            # Librosa lpc expects 1D array\n",
    "            a_lpc = librosa.lpc(x, order=4)\n",
    "            for i, c in enumerate(a_lpc[1:]): # Skip a[0] which is 1\n",
    "                f[f'lpc_coeff_{i}'] = c\n",
    "            lpc_resid = signal.lfilter(a_lpc, 1, x)\n",
    "            f['lpc_resid_std'] = np.std(lpc_resid)\n",
    "            f['lpc_resid_var'] = np.var(lpc_resid)\n",
    "            f['lpc_resid_energy'] = np.sum(lpc_resid**2)\n",
    "        except:\n",
    "            for i in range(4): f[f'lpc_coeff_{i}'] = 0\n",
    "            \n",
    "        return f\n",
    "\n",
    "    def extract_frequency_domain(self, x):\n",
    "        f = {}\n",
    "        # FFT / PSD\n",
    "        freqs, psd = signal.welch(x, self.sfreq, nperseg=min(len(x), 256))\n",
    "        f['dominant_freq'] = freqs[np.argmax(psd)]\n",
    "        f['total_power'] = np.sum(psd)\n",
    "        \n",
    "        norm_peak, norm_harm, norm_total = self._normalized_peak_harmonic_energy(\n",
    "        freqs, psd, n_harmonics=3, band_width=0.5)\n",
    "        f['norm_peak_band_energy'] = norm_peak\n",
    "        f['norm_harmonics_energy'] = norm_harm\n",
    "        f['norm_peak_plus_harmonics_energy'] = norm_total\n",
    "        \n",
    "        # MFCC (Mel-frequency cepstral coefficients)\n",
    "        # Treating ECG as \"audio\" for texture analysis is common\n",
    "        try:\n",
    "            sig_len = len(x)\n",
    "            # Choose an n_fft that is not larger than the signal and is reasonably sized\n",
    "            n_fft = min(1024, sig_len)\n",
    "            hop_length = max(1, n_fft // 4)\n",
    "\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=x.astype(float),\n",
    "                sr=self.sfreq,\n",
    "                n_mfcc=5,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "            )\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            for i, c in enumerate(mfcc_mean):\n",
    "                f[f'mfcc_{i}'] = c\n",
    "        except Exception:\n",
    "            for i in range(5):\n",
    "                f[f'mfcc_{i}'] = 0\n",
    "\n",
    "            \n",
    "        # Discrete Cosine Transform (DCT)\n",
    "        dct_val = fftpack.dct(x, type=2, norm='ortho')\n",
    "        f['dct_mean'] = np.mean(np.abs(dct_val))\n",
    "        f['dct_var'] = np.var(dct_val)\n",
    "        \n",
    "        # Hilbert Features\n",
    "        analytic = signal.hilbert(x)\n",
    "        amplitude = np.abs(analytic)\n",
    "        phase = np.unwrap(np.angle(analytic))\n",
    "        inst_freq = (np.diff(phase) / (2.0*np.pi) * self.sfreq)\n",
    "        f['hilbert_amp_mean'] = np.mean(amplitude)\n",
    "        f['hilbert_freq_mean'] = np.mean(inst_freq)\n",
    "        \n",
    "        return f\n",
    "\n",
    "    def extract_time_frequency_domain(self, x):\n",
    "        f = {}\n",
    "        # STFT / Spectrogram features\n",
    "        # (Used as proxy for heavy WVD/CWD computations)\n",
    "        f_stft, t_stft, Zxx = signal.stft(x, fs=self.sfreq, nperseg=min(len(x)//4, 64))\n",
    "        Sxx = np.abs(Zxx)\n",
    "        \n",
    "        f['stft_energy_mean'] = np.mean(Sxx)\n",
    "        f['stft_energy_std'] = np.std(Sxx)\n",
    "        \n",
    "        # Peak frequency tracking\n",
    "        peak_freqs = f_stft[np.argmax(Sxx, axis=0)]\n",
    "        f['stft_peak_freq_mean'] = np.mean(peak_freqs)\n",
    "        \n",
    "        return f\n",
    "\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        f = {}\n",
    "        # Wavelet Transform (DWT)\n",
    "        coeffs = pywt.wavedec(x, 'db4', level=4)\n",
    "        coeff_names = ['A4', 'D4', 'D3', 'D2', 'D1']\n",
    "        for name, c in zip(coeff_names, coeffs):\n",
    "            f[f'dwt_{name}_energy'] = np.sum(c**2)\n",
    "            f[f'dwt_{name}_std'] = np.std(c)\n",
    "            \n",
    "        # SVD (Singular Value Decomposition)\n",
    "        # SVD on a 1D signal is just scalar, usually done on trajectory matrix (Hankel)\n",
    "        # We'll do SVD on the spectrogram matrix Sxx for robustness\n",
    "        try:\n",
    "            _, _, Zxx = signal.stft(x, fs=self.sfreq, nperseg=64)\n",
    "            Sxx = np.abs(Zxx)\n",
    "            U, s, Vh = np.linalg.svd(Sxx, full_matrices=False)\n",
    "            # Top 3 Singular Values\n",
    "            for i in range(min(3, len(s))):\n",
    "                f[f'svd_val_{i}'] = s[i]\n",
    "        except:\n",
    "            for i in range(3): f[f'svd_val_{i}'] = 0\n",
    "            \n",
    "        return f\n",
    "    \n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_time_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "        \n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "print(\"ECGFeatureExtractor class defined.\")\n",
    "\n",
    "extractor = ECGFeatureExtractor()\n",
    "dummy = np.random.randn(1000)  # 4 seconds at 250Hz\n",
    "\n",
    "_ = extractor.list_feature_names(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a3a8cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMGFeatureExtractor class defined.\n",
      "Total feature count: 78\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "max\n",
      "min\n",
      "range\n",
      "iqr\n",
      "kurtosis\n",
      "skewness\n",
      "Q1\n",
      "Q3\n",
      "integrated_EMG\n",
      "mean_absolute_value\n",
      "simple_square_integral\n",
      "root_mean_square\n",
      "RMS\n",
      "variance\n",
      "waveform_length\n",
      "difference_absolute_mean_value\n",
      "difference_variance\n",
      "difference_absolute_standard_deviation\n",
      "integrated_absolute_second_derivative\n",
      "integrated_absolute_third_derivative\n",
      "integrated_exponential_absolute\n",
      "integrated_absolute_log\n",
      "integrated_exponential\n",
      "second_order_moment\n",
      "Willison_amplitude\n",
      "myopulse_percentage_rate\n",
      "zero_crossings\n",
      "slope_sign_changes\n",
      "Hjorth_activity\n",
      "Hjorth_mobility\n",
      "Hjorth_complexity\n",
      "FFT_mean_mag\n",
      "AR_order4_coeff_0\n",
      "AR_order4_coeff_1\n",
      "AR_order4_coeff_2\n",
      "AR_order4_coeff_3\n",
      "AR_order4_PSD_est\n",
      "AR_order7_coeff_0\n",
      "AR_order7_coeff_1\n",
      "AR_order7_coeff_2\n",
      "AR_order7_coeff_3\n",
      "AR_order7_coeff_4\n",
      "AR_order7_coeff_5\n",
      "AR_order7_coeff_6\n",
      "AR_order7_PSD_est\n",
      "wavelet_A4_SA_sum\n",
      "wavelet_A4_SD_std\n",
      "wavelet_A4_VR_var\n",
      "wavelet_A4_CM_moment4\n",
      "wavelet_A4_SK_skew\n",
      "wavelet_A4_KU_kurt\n",
      "wavelet_D4_SA_sum\n",
      "wavelet_D4_SD_std\n",
      "wavelet_D4_VR_var\n",
      "wavelet_D4_CM_moment4\n",
      "wavelet_D4_SK_skew\n",
      "wavelet_D4_KU_kurt\n",
      "wavelet_D3_SA_sum\n",
      "wavelet_D3_SD_std\n",
      "wavelet_D3_VR_var\n",
      "wavelet_D3_CM_moment4\n",
      "wavelet_D3_SK_skew\n",
      "wavelet_D3_KU_kurt\n",
      "wavelet_D2_SA_sum\n",
      "wavelet_D2_SD_std\n",
      "wavelet_D2_VR_var\n",
      "wavelet_D2_CM_moment4\n",
      "wavelet_D2_SK_skew\n",
      "wavelet_D2_KU_kurt\n",
      "wavelet_D1_SA_sum\n",
      "wavelet_D1_SD_std\n",
      "wavelet_D1_VR_var\n",
      "wavelet_D1_CM_moment4\n",
      "wavelet_D1_SK_skew\n",
      "wavelet_D1_KU_kurt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "import pywt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "class EMGFeatureExtractor:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    def extract_time_domain(self, x):\n",
    "        f = {}\n",
    "        # --- 1. Basic Statistics ---\n",
    "        f['mean'] = np.mean(x)\n",
    "        f['std'] = np.std(x)\n",
    "        f['median'] = np.median(x)\n",
    "        f['max'] = np.max(x)\n",
    "        f['min'] = np.min(x)\n",
    "        f['range'] = f['max'] - f['min']\n",
    "        f['iqr'] = stats.iqr(x)\n",
    "        f['kurtosis'] = stats.kurtosis(x)\n",
    "        f['skewness'] = stats.skew(x)\n",
    "        f['Q1'] = np.percentile(x, 25)\n",
    "        f['Q3'] = np.percentile(x, 75)\n",
    "        \n",
    "        # --- 2. Conventional EMG Features ---\n",
    "        # Integrated EMG (IEMG): Sum of absolute values\n",
    "        f['integrated_EMG'] = np.sum(np.abs(x))\n",
    "        \n",
    "        # Mean Absolute Value (MAV)\n",
    "        f['mean_absolute_value'] = np.mean(np.abs(x))\n",
    "        \n",
    "        # Simple Square Integral (SSI)\n",
    "        f['simple_square_integral'] = np.sum(x**2)\n",
    "        \n",
    "        # Root Mean Square (RMS)\n",
    "        f['root_mean_square'] = np.sqrt(np.mean(x**2))\n",
    "        f['RMS'] = f['root_mean_square'] # Duplicate as requested\n",
    "        \n",
    "        # Variance\n",
    "        f['variance'] = np.var(x)\n",
    "        \n",
    "        # Waveform Length (WL): Sum of absolute differences\n",
    "        diff1 = np.diff(x)\n",
    "        f['waveform_length'] = np.sum(np.abs(diff1))\n",
    "        \n",
    "        # Difference Absolute Mean Value (DAMV) / Mean Absolute Deviation\n",
    "        f['difference_absolute_mean_value'] = np.mean(np.abs(diff1))\n",
    "        \n",
    "        # Difference Variance\n",
    "        f['difference_variance'] = np.var(diff1)\n",
    "        \n",
    "        # Difference Absolute Standard Deviation (DASD)\n",
    "        f['difference_absolute_standard_deviation'] = np.std(diff1)\n",
    "        \n",
    "        f['integrated_absolute_second_derivative'] = np.sum(np.abs(np.diff(diff1)))\n",
    "        f['integrated_absolute_third_derivative'] = np.sum(np.abs(np.diff(np.diff(diff1))))\n",
    "        f['integrated_exponential_absolute'] = np.sum(np.exp(np.abs(x)))\n",
    "        f['integrated_absolute_log'] = np.sum(np.log(np.abs(x) + 1e-6))\n",
    "        f['integrated_exponential'] = np.sum(np.exp(x))\n",
    "        \n",
    "        # Second Order Moment\n",
    "        f['second_order_moment'] = np.mean(x**2)\n",
    "        \n",
    "        # Willison Amplitude (WAMP): Count differences > threshold\n",
    "        # Threshold is usually dataset dependent. Using a small fixed value or heuristic.\n",
    "        # For normalized data, 0.1 might be appropriate. For raw, usually 10mV+. \n",
    "        # We'll use a heuristic: 10% of std dev.\n",
    "        threshold = 0.1 * f['std']\n",
    "        f['Willison_amplitude'] = np.sum(np.abs(diff1) > threshold)\n",
    "        \n",
    "        # Myopulse Percentage Rate (MYOP): Fraction of signal > threshold\n",
    "        f['myopulse_percentage_rate'] = np.sum(np.abs(x) > threshold) / len(x)\n",
    "\n",
    "        # --- 3. Counts / Changes ---\n",
    "        # Zero Crossings\n",
    "        # (Counts times signal crosses mean)\n",
    "        centered = x - f['mean']\n",
    "        f['zero_crossings'] = np.sum(np.diff(np.signbit(centered).astype(int)) != 0)\n",
    "        \n",
    "        # Slope Sign Changes (SSC)\n",
    "        # Change in slope direction (convex to concave or vice versa)\n",
    "        # (x[i] - x[i-1]) * (x[i+1] - x[i]) < 0\n",
    "        f['slope_sign_changes'] = np.sum(np.diff(np.sign(diff1)) != 0)\n",
    "        \n",
    "        # --- 4. Hjorth Parameters ---\n",
    "        # Activity (Variance)\n",
    "        f['Hjorth_activity'] = f['variance']\n",
    "        \n",
    "        # Mobility: sqrt(var(diff) / var(x))\n",
    "        if f['variance'] > 0:\n",
    "            f['Hjorth_mobility'] = np.sqrt(np.var(diff1) / f['variance'])\n",
    "        else:\n",
    "            f['Hjorth_mobility'] = 0\n",
    "            \n",
    "        # Complexity: mobility(diff) / mobility(x)\n",
    "        diff2 = np.diff(diff1)\n",
    "        if f['Hjorth_mobility'] > 0 and np.var(diff1) > 0:\n",
    "            mob_diff = np.sqrt(np.var(diff2) / np.var(diff1))\n",
    "            f['Hjorth_complexity'] = mob_diff / f['Hjorth_mobility']\n",
    "        else:\n",
    "            f['Hjorth_complexity'] = 0\n",
    "\n",
    "        # --- 5. Higher Order / Integral Proxies ---\n",
    "        # Integrated Exponential: sum(exp(x))\n",
    "        # (Careful with overflow on raw data, best on normalized or small window)\n",
    "        try:\n",
    "            f['integrated_exponential'] = np.sum(np.exp(np.clip(x, -5, 5)))\n",
    "        except:\n",
    "            f['integrated_exponential'] = 0\n",
    "\n",
    "        f['integrated_absolute_log'] = np.sum(np.log(np.abs(x) + 1e-6))\n",
    "\n",
    "        return f\n",
    "\n",
    "    def extract_frequency_domain(self, x):\n",
    "        f = {}\n",
    "        # FFT Coefficients (mean magnitude)\n",
    "        fft_vals = np.fft.rfft(x)\n",
    "        f['FFT_mean_mag'] = np.mean(np.abs(fft_vals))\n",
    "        \n",
    "        # AR Models (Order 4 and 7)\n",
    "        for order in [4, 7]:\n",
    "            try:\n",
    "                res = AutoReg(x, lags=order, trend='n').fit()\n",
    "                for i, c in enumerate(res.params):\n",
    "                    f[f'AR_order{order}_coeff_{i}'] = c\n",
    "                # PSD proxy from AR residuals\n",
    "                f[f'AR_order{order}_PSD_est'] = np.var(res.resid)\n",
    "            except:\n",
    "                for i in range(order): f[f'AR_order{order}_coeff_{i}'] = 0\n",
    "                f[f'AR_order{order}_PSD_est'] = 0\n",
    "                \n",
    "        return f\n",
    "\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        f = {}\n",
    "        # Wavelet Transform (DWT)\n",
    "        # Using 'db4' level 4\n",
    "        coeffs = pywt.wavedec(x, 'db4', level=4)\n",
    "        coeff_names = ['A4', 'D4', 'D3', 'D2', 'D1']\n",
    "        \n",
    "        for name, c in zip(coeff_names, coeffs):\n",
    "            f[f'wavelet_{name}_SA_sum'] = np.sum(np.abs(c))\n",
    "            f[f'wavelet_{name}_SD_std'] = np.std(c)\n",
    "            f[f'wavelet_{name}_VR_var'] = np.var(c)\n",
    "            # Fourth Moment (unnormalized Kurtosis-like)\n",
    "            f[f'wavelet_{name}_CM_moment4'] = np.mean(c**4)\n",
    "            f[f'wavelet_{name}_SK_skew'] = stats.skew(c)\n",
    "            f[f'wavelet_{name}_KU_kurt'] = stats.kurtosis(c)\n",
    "            \n",
    "        return f\n",
    "    \n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "        \n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "print(\"EMGFeatureExtractor class defined.\")\n",
    "\n",
    "extractor = EMGFeatureExtractor()\n",
    "dummy = np.random.randn(1000)  # 4 seconds at 250Hz\n",
    "\n",
    "_ = extractor.list_feature_names(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ae755be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVFeatureExtractor classes defined.\n",
      "\n",
      "--- TRIAXIAL FEATURES ---\n",
      "Total feature count: 35\n",
      "Feature names:\n",
      "triax_mean_X\n",
      "triax_mean_Y\n",
      "triax_mean_Z\n",
      "triax_variance_X\n",
      "triax_variance_Y\n",
      "triax_variance_Z\n",
      "triax_total_sum_vector\n",
      "triax_sum_vector_magnitude\n",
      "triax_activity_single_magnitude_area\n",
      "triax_dynamic_sum_vector\n",
      "triax_vertical_acceleration\n",
      "triax_angular_velocity_aggregate\n",
      "triax_fft_mean_mag\n",
      "triax_fft_energy\n",
      "triax_dominant_freq\n",
      "triax_wavelet_A4_energy\n",
      "triax_wavelet_A4_mean\n",
      "triax_wavelet_A4_std\n",
      "triax_wavelet_A4_skew\n",
      "triax_wavelet_D4_energy\n",
      "triax_wavelet_D4_mean\n",
      "triax_wavelet_D4_std\n",
      "triax_wavelet_D4_skew\n",
      "triax_wavelet_D3_energy\n",
      "triax_wavelet_D3_mean\n",
      "triax_wavelet_D3_std\n",
      "triax_wavelet_D3_skew\n",
      "triax_wavelet_D2_energy\n",
      "triax_wavelet_D2_mean\n",
      "triax_wavelet_D2_std\n",
      "triax_wavelet_D2_skew\n",
      "triax_wavelet_D1_energy\n",
      "triax_wavelet_D1_mean\n",
      "triax_wavelet_D1_std\n",
      "triax_wavelet_D1_skew\n",
      "\n",
      "--- 1D FEATURES ---\n",
      "Total feature count: 44\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "max\n",
      "min\n",
      "range\n",
      "iqr\n",
      "kurtosis\n",
      "skewness\n",
      "Q1\n",
      "Q3\n",
      "signal_magnitude_area\n",
      "zero_crossing_rate\n",
      "peak_count\n",
      "val_first_minus_last\n",
      "val_first_minus_max\n",
      "val_first_minus_min\n",
      "val_last_minus_max\n",
      "val_last_minus_min\n",
      "rank_last_value\n",
      "rank_min_value\n",
      "fft_mean_mag\n",
      "fft_energy\n",
      "dominant_freq\n",
      "wavelet_A4_energy\n",
      "wavelet_A4_mean\n",
      "wavelet_A4_std\n",
      "wavelet_A4_skew\n",
      "wavelet_D4_energy\n",
      "wavelet_D4_mean\n",
      "wavelet_D4_std\n",
      "wavelet_D4_skew\n",
      "wavelet_D3_energy\n",
      "wavelet_D3_mean\n",
      "wavelet_D3_std\n",
      "wavelet_D3_skew\n",
      "wavelet_D2_energy\n",
      "wavelet_D2_mean\n",
      "wavelet_D2_std\n",
      "wavelet_D2_skew\n",
      "wavelet_D1_energy\n",
      "wavelet_D1_mean\n",
      "wavelet_D1_std\n",
      "wavelet_D1_skew\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "import scipy.fftpack as fftpack\n",
    "import pywt\n",
    "\n",
    "class MOVTriaxFeatureExtractor:\n",
    "    def __init__(self, sfreq=250, vertical_axis=2):\n",
    "        \"\"\"\n",
    "        Triaxial MOV feature extractor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sfreq : float\n",
    "            Sampling frequency (Hz).\n",
    "        vertical_axis : int\n",
    "            Index of the vertical acceleration axis (0, 1, or 2).\n",
    "        \"\"\"\n",
    "        self.sfreq = sfreq\n",
    "        self.vertical_axis = vertical_axis\n",
    "\n",
    "    # --- Helper: ensure (N, 3) shape ---\n",
    "    def _ensure_triax_shape(self, arr, name=\"acc\"):\n",
    "        \"\"\"\n",
    "        Accepts (N, 3) or (3, N) and returns (N, 3).\n",
    "        \"\"\"\n",
    "        arr = np.asarray(arr)\n",
    "        if arr.ndim != 2 or 3 not in arr.shape:\n",
    "            raise ValueError(f\"{name} must be 2D with one dimension = 3, got shape {arr.shape}\")\n",
    "        if arr.shape[1] == 3:\n",
    "            return arr\n",
    "        else:\n",
    "            # assume (3, N)\n",
    "            return arr.T\n",
    "\n",
    "    # ---------- TIME DOMAIN ----------\n",
    "    def extract_time_domain(self, acc, gyro=None):\n",
    "        \"\"\"\n",
    "        acc : array-like, shape (N, 3) or (3, N)\n",
    "        gyro : array-like, shape (N, 3) or (3, N), optional\n",
    "        \"\"\"\n",
    "        f = {}\n",
    "\n",
    "        acc = self._ensure_triax_shape(acc, name=\"acc\")\n",
    "        ax, ay, az = acc[:, 0], acc[:, 1], acc[:, 2]\n",
    "\n",
    "        # --- Axis-wise stats ---\n",
    "        f['triax_mean_X'] = float(np.mean(ax))\n",
    "        f['triax_mean_Y'] = float(np.mean(ay))\n",
    "        f['triax_mean_Z'] = float(np.mean(az))\n",
    "\n",
    "        f['triax_variance_X'] = float(np.var(ax))\n",
    "        f['triax_variance_Y'] = float(np.var(ay))\n",
    "        f['triax_variance_Z'] = float(np.var(az))\n",
    "\n",
    "        # --- Resultant magnitude ---\n",
    "        mag = np.sqrt(ax**2 + ay**2 + az**2)\n",
    "\n",
    "        # Total sum of vector magnitude over the window\n",
    "        f['triax_total_sum_vector'] = float(np.sum(mag))\n",
    "\n",
    "        # Mean magnitude (average length of the sum vector)\n",
    "        f['triax_sum_vector_magnitude'] = float(np.mean(mag))\n",
    "\n",
    "        # Activity / single magnitude area: sum of |ax|+|ay|+|az|\n",
    "        f['triax_activity_single_magnitude_area'] = float(\n",
    "            np.sum(np.abs(ax) + np.abs(ay) + np.abs(az))\n",
    "        )\n",
    "\n",
    "        # --- Dynamic component (subtract DC/gravity per axis) ---\n",
    "        ax_d = ax - np.mean(ax)\n",
    "        ay_d = ay - np.mean(ay)\n",
    "        az_d = az - np.mean(az)\n",
    "        mag_dyn = np.sqrt(ax_d**2 + ay_d**2 + az_d**2)\n",
    "        f['triax_dynamic_sum_vector'] = float(np.sum(mag_dyn))\n",
    "\n",
    "        # --- Vertical acceleration ---\n",
    "        if self.vertical_axis not in (0, 1, 2):\n",
    "            raise ValueError(\"vertical_axis must be 0, 1, or 2\")\n",
    "        a_vert = acc[:, self.vertical_axis]\n",
    "        f['triax_vertical_acceleration'] = float(np.mean(np.abs(a_vert)))\n",
    "\n",
    "        # --- Angular velocity aggregate (if gyro provided) ---\n",
    "        if gyro is not None:\n",
    "            gyro = self._ensure_triax_shape(gyro, name=\"gyro\")\n",
    "            gx, gy, gz = gyro[:, 0], gyro[:, 1], gyro[:, 2]\n",
    "            gyro_mag = np.sqrt(gx**2 + gy**2 + gz**2)\n",
    "            # Aggregate = mean magnitude over window\n",
    "            f['triax_angular_velocity_aggregate'] = float(np.mean(gyro_mag))\n",
    "        else:\n",
    "            # If no gyro channel, set to 0 (or np.nan if you prefer)\n",
    "            f['triax_angular_velocity_aggregate'] = 0.0\n",
    "\n",
    "        return f\n",
    "\n",
    "    # ---------- FREQUENCY DOMAIN ----------\n",
    "    def extract_frequency_domain(self, acc):\n",
    "        \"\"\"\n",
    "        Basic frequency-domain features from resultant magnitude.\n",
    "        \"\"\"\n",
    "        f = {}\n",
    "\n",
    "        acc = self._ensure_triax_shape(acc, name=\"acc\")\n",
    "        ax, ay, az = acc[:, 0], acc[:, 1], acc[:, 2]\n",
    "        mag = np.sqrt(ax**2 + ay**2 + az**2)\n",
    "\n",
    "        # FFT on magnitude (similar to 1D)\n",
    "        fft_vals = np.fft.rfft(mag)\n",
    "        freqs = np.fft.rfftfreq(len(mag), d=1/self.sfreq)\n",
    "\n",
    "        f['triax_fft_mean_mag'] = float(np.mean(np.abs(fft_vals)))\n",
    "        f['triax_fft_energy'] = float(np.sum(np.abs(fft_vals)**2))\n",
    "        f['triax_dominant_freq'] = float(freqs[np.argmax(np.abs(fft_vals))])\n",
    "\n",
    "        return f\n",
    "\n",
    "    # ---------- DECOMPOSITION DOMAIN ----------\n",
    "    def extract_decomposition_domain(self, acc):\n",
    "        \"\"\"\n",
    "        Wavelet features on resultant magnitude (parallel to 1D extractor).\n",
    "        \"\"\"\n",
    "        f = {}\n",
    "\n",
    "        acc = self._ensure_triax_shape(acc, name=\"acc\")\n",
    "        ax, ay, az = acc[:, 0], acc[:, 1], acc[:, 2]\n",
    "        mag = np.sqrt(ax**2 + ay**2 + az**2)\n",
    "\n",
    "        coeffs = pywt.wavedec(mag, 'db4', level=4)\n",
    "        coeff_names = ['A4', 'D4', 'D3', 'D2', 'D1']\n",
    "\n",
    "        for name, c in zip(coeff_names, coeffs):\n",
    "            f[f'triax_wavelet_{name}_energy'] = float(np.sum(c**2))\n",
    "            f[f'triax_wavelet_{name}_mean'] = float(np.mean(c))\n",
    "            f[f'triax_wavelet_{name}_std'] = float(np.std(c))\n",
    "            f[f'triax_wavelet_{name}_skew'] = float(stats.skew(c))\n",
    "\n",
    "        return f\n",
    "    \n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "        \n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "\n",
    "\n",
    "class MOV1DFeatureExtractor:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    def extract_time_domain(self, x):\n",
    "        f = {}\n",
    "        # --- 1. Basic Statistics ---\n",
    "        f['mean'] = np.mean(x)\n",
    "        f['std'] = np.std(x)\n",
    "        f['median'] = np.median(x)\n",
    "        f['max'] = np.max(x)\n",
    "        f['min'] = np.min(x)\n",
    "        f['range'] = f['max'] - f['min']\n",
    "        f['iqr'] = stats.iqr(x)\n",
    "        f['kurtosis'] = stats.kurtosis(x)\n",
    "        f['skewness'] = stats.skew(x)\n",
    "        f['Q1'] = np.percentile(x, 25)\n",
    "        f['Q3'] = np.percentile(x, 75)\n",
    "        \n",
    "        # --- 2. Temporal Features ---\n",
    "        # Signal Magnitude Area (SMA) - usually sum of absolute values\n",
    "        f['signal_magnitude_area'] = np.sum(np.abs(x))\n",
    "        \n",
    "        # Zero Crossing Rate\n",
    "        centered = x - f['mean']\n",
    "        f['zero_crossing_rate'] = np.sum(np.diff(np.signbit(centered).astype(int)) != 0) / len(x)\n",
    "        \n",
    "        # Peak Count (simple peak detection)\n",
    "        # Prominence heuristic: 0.5 * std\n",
    "        peaks, _ = signal.find_peaks(x, prominence=0.5*f['std'])\n",
    "        f['peak_count'] = len(peaks)\n",
    "        \n",
    "        # --- 3. Trajectory / Angular Velocity Features ---\n",
    "        # (First-Last, First-Max, etc.)\n",
    "        first = x[0]\n",
    "        last = x[-1]\n",
    "        mx = f['max']\n",
    "        mn = f['min']\n",
    "        \n",
    "        f['val_first_minus_last'] = first - last\n",
    "        f['val_first_minus_max'] = first - mx\n",
    "        f['val_first_minus_min'] = first - mn\n",
    "        f['val_last_minus_max'] = last - mx\n",
    "        f['val_last_minus_min'] = last - mn\n",
    "        \n",
    "        # --- 4. Acceleration Order (Ranking) ---\n",
    "        # Rank of the last value in the sorted array (0 to 1)\n",
    "        sorted_x = np.sort(x)\n",
    "        f['rank_last_value'] = np.searchsorted(sorted_x, last) / len(x)\n",
    "        f['rank_min_value'] = 0.0 # By definition min is rank 0\n",
    "        \n",
    "        return f\n",
    "\n",
    "    def extract_frequency_domain(self, x):\n",
    "        f = {}\n",
    "        # FFT / DFT Coefficients\n",
    "        # Mean magnitude of coefficients\n",
    "        fft_vals = np.fft.rfft(x)\n",
    "        f['fft_mean_mag'] = np.mean(np.abs(fft_vals))\n",
    "        f['fft_energy'] = np.sum(np.abs(fft_vals)**2)\n",
    "        \n",
    "        # Dominant Frequency\n",
    "        freqs = np.fft.rfftfreq(len(x), d=1/self.sfreq)\n",
    "        f['dominant_freq'] = freqs[np.argmax(np.abs(fft_vals))]\n",
    "        \n",
    "        return f\n",
    "\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        f = {}\n",
    "        # Wavelet Transform (DWT)\n",
    "        coeffs = pywt.wavedec(x, 'db4', level=4)\n",
    "        coeff_names = ['A4', 'D4', 'D3', 'D2', 'D1']\n",
    "        \n",
    "        for name, c in zip(coeff_names, coeffs):\n",
    "            f[f'wavelet_{name}_energy'] = np.sum(c**2)\n",
    "            f[f'wavelet_{name}_mean'] = np.mean(c)\n",
    "            f[f'wavelet_{name}_std'] = np.std(c)\n",
    "            f[f'wavelet_{name}_skew'] = stats.skew(c)\n",
    "            \n",
    "        return f\n",
    "    \n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "        \n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "print(\"MOVFeatureExtractor classes defined.\")\n",
    "\n",
    "# ---- Dummy test for MOVTriaxFeatureExtractor ----\n",
    "\n",
    "# Create dummy triaxial accelerometer data: 1000 samples of (x,y,z)\n",
    "np.random.seed(0)\n",
    "acc_dummy = np.random.randn(1000, 3)  # shape (1000, 3)\n",
    "\n",
    "# Create dummy gyro data (optional)\n",
    "gyro_dummy = np.random.randn(1000, 3)\n",
    "\n",
    "# Instantiate extractor\n",
    "triax_extractor = MOVTriaxFeatureExtractor(sfreq=250, vertical_axis=2)\n",
    "\n",
    "print(\"\\n--- TRIAXIAL FEATURES ---\")\n",
    "_ = triax_extractor.list_feature_names(acc_dummy)\n",
    "\n",
    "\n",
    "# ---- Dummy test for MOV1DFeatureExtractor ----\n",
    "\n",
    "# Create dummy 1D motion signal\n",
    "x_dummy = np.random.randn(1000)\n",
    "\n",
    "# Instantiate extractor\n",
    "mov1d_extractor = MOV1DFeatureExtractor(sfreq=250)\n",
    "\n",
    "print(\"\\n--- 1D FEATURES ---\")\n",
    "_ = mov1d_extractor.list_feature_names(x_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c2f02b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2844 files to process with Fixed MOV logic.\n",
      "Starting Parallel Feature Extraction (Fixed MOV=6)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 58.4min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 158\u001b[0m\n\u001b[0;32m    155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Run Parallel\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file_expanded_fixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meeg_ex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mecg_ex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memg_ex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmov1d_ex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmov_triax_ex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mov_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnpz_files\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Consolidate\u001b[39;00m\n\u001b[0;32m    166\u001b[0m X_expanded_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\torchenv\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\torchenv\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\torchenv\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure OUTPUT_DIR is defined\n",
    "OUTPUT_DIR = Path(\"F:\\Rice\\Rice F25\\Seizure Project\\Processed_Gemini_V5_Augmented\")  # Update as needed\n",
    "BASE_PATH = Path(\"F:\\Rice\\Rice F25\\Seizure Project\")\n",
    "FEATURE_DIR = BASE_PATH / 'Features'\n",
    "FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_channel_features(extractor, signal_data):\n",
    "    \"\"\"\n",
    "    Helper to run all extraction methods on a single 1D channel\n",
    "    and return a sorted feature vector to ensure consistency.\n",
    "    Works for EEG/ECG/EMG/MOV1D extractors.\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    try:\n",
    "        # Time Domain\n",
    "        feats.update(extractor.extract_time_domain(signal_data))\n",
    "        # Frequency Domain\n",
    "        feats.update(extractor.extract_frequency_domain(signal_data))\n",
    "        # Time-Frequency Domain (if available)\n",
    "        if hasattr(extractor, 'extract_time_frequency_domain'):\n",
    "            feats.update(extractor.extract_time_frequency_domain(signal_data))\n",
    "        # Decomposition Domain\n",
    "        feats.update(extractor.extract_decomposition_domain(signal_data))\n",
    "\n",
    "        # Return values sorted by key for consistent column ordering\n",
    "        return np.array([feats[k] for k in sorted(feats.keys())])\n",
    "    except Exception as e:\n",
    "        # Fallback: 1-dim zero marker if something goes wrong\n",
    "        # (Ideally we should handle this better, but for now return zeros matching expected size roughly or just a flag)\n",
    "        return np.zeros(1)\n",
    "\n",
    "\n",
    "def extract_triax_mov_features(triax_extractor, acc_3axis, gyro_3axis=None):\n",
    "    \"\"\"\n",
    "    Helper to run all extraction methods on triaxial MOV data\n",
    "    (acc: (N,3), optional gyro: (N,3)) and return a sorted feature vector.\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    try:\n",
    "        # Time Domain (uses both acc + gyro if provided)\n",
    "        feats.update(triax_extractor.extract_time_domain(acc_3axis, gyro=gyro_3axis))\n",
    "        # Frequency Domain (on acc magnitude)\n",
    "        feats.update(triax_extractor.extract_frequency_domain(acc_3axis))\n",
    "        # Decomposition Domain (on acc magnitude)\n",
    "        feats.update(triax_extractor.extract_decomposition_domain(acc_3axis))\n",
    "\n",
    "        return np.array([feats[k] for k in sorted(feats.keys())])\n",
    "    except Exception:\n",
    "        return np.zeros(1)\n",
    "\n",
    "\n",
    "def process_file_expanded_fixed(\n",
    "    npz_file,\n",
    "    eeg_ext,\n",
    "    ecg_ext,\n",
    "    emg_ext,\n",
    "    mov1d_ext,\n",
    "    mov_triax_ext,\n",
    "    n_mov_channels=6\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads an NPZ file, extracts features with FIXED MOV channel count.\n",
    "    Corrected to handle X_student_bio and X_student_mov separately.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = np.load(npz_file)\n",
    "        X_t = data['X_teacher']       # Shape: (n_win, 2, 500)\n",
    "        X_s_bio = data['X_student_bio'] # Shape: (n_win, 2, 500) -> ECG, EMG\n",
    "        X_s_mov = data['X_student_mov'] # Shape: (n_win, 6, 50) -> 3 Acc, 3 Gyro\n",
    "        y   = data['y']\n",
    "\n",
    "        n_wins = X_t.shape[0]\n",
    "        file_features = []\n",
    "\n",
    "        for i in range(n_wins):\n",
    "            win_feats = []\n",
    "\n",
    "            # --- 1. EEG Features (Teacher) ---\n",
    "            for ch_idx in range(X_t.shape[1]):\n",
    "                win_feats.append(extract_channel_features(eeg_ext, X_t[i, ch_idx]))\n",
    "\n",
    "            # --- 2. Student Bio Features ---\n",
    "            # Index 0: ECG\n",
    "            # Index 1: EMG\n",
    "            \n",
    "            # ECG\n",
    "            if X_s_bio.shape[1] > 0:\n",
    "                win_feats.append(extract_channel_features(ecg_ext, X_s_bio[i, 0]))\n",
    "            \n",
    "            # EMG\n",
    "            if X_s_bio.shape[1] > 1:\n",
    "                win_feats.append(extract_channel_features(emg_ext, X_s_bio[i, 1]))\n",
    "\n",
    "            # --- 3. MOV 1D Features (per channel) ---\n",
    "            # X_s_mov has shape (n_win, 6, 50)\n",
    "            # Channels 0-2: Accel X,Y,Z\n",
    "            # Channels 3-5: Gyro X,Y,Z\n",
    "\n",
    "            for k in range(n_mov_channels):\n",
    "                if k < X_s_mov.shape[1]:\n",
    "                    feat_vec = extract_channel_features(mov1d_ext, X_s_mov[i, k])\n",
    "                else:\n",
    "                    # Zero padding if fewer channels than expected\n",
    "                    dummy_signal = np.zeros(X_s_mov.shape[2]) \n",
    "                    feat_vec = extract_channel_features(mov1d_ext, dummy_signal)\n",
    "                \n",
    "                win_feats.append(feat_vec)\n",
    "\n",
    "            # --- 4. MOV Triax Features ---\n",
    "            # Accel\n",
    "            acc_3axis = np.zeros((X_s_mov.shape[2], 3))\n",
    "            if X_s_mov.shape[1] >= 3:\n",
    "                acc_3axis = X_s_mov[i, 0:3].T # Transpose to (N, 3)\n",
    "            \n",
    "            # Gyro\n",
    "            gyro_3axis = np.zeros((X_s_mov.shape[2], 3))\n",
    "            if X_s_mov.shape[1] >= 6:\n",
    "                gyro_3axis = X_s_mov[i, 3:6].T # Transpose to (N, 3)\n",
    "\n",
    "            # Extract triax MOV features\n",
    "            triax_feats = extract_triax_mov_features(mov_triax_ext, acc_3axis, gyro_3axis)\n",
    "            win_feats.append(triax_feats)\n",
    "\n",
    "            # Concatenate all features for this window\n",
    "            file_features.append(np.concatenate(win_feats))\n",
    "\n",
    "        return np.array(file_features), y\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {npz_file.name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Re-verify Extractors (Should exist from previous cells)\n",
    "if 'eeg_ex' not in locals() or 'mov1d_ex' not in locals() or 'mov_triax_ex' not in locals():\n",
    "    print(\"Re-initializing extractors...\")\n",
    "    eeg_ex       = EEGFeatureExtractor(sfreq=250)\n",
    "    ecg_ex       = ECGFeatureExtractor(sfreq=250)\n",
    "    emg_ex       = EMGFeatureExtractor(sfreq=250)\n",
    "    # Note: MOV data is 25Hz (50 samples / 2s)\n",
    "    mov1d_ex     = MOV1DFeatureExtractor(sfreq=25) \n",
    "    mov_triax_ex = MOVTriaxFeatureExtractor(sfreq=25, vertical_axis=2)\n",
    "\n",
    "if OUTPUT_DIR.exists():\n",
    "    npz_files = sorted(list(OUTPUT_DIR.glob(\"*.npz\")))\n",
    "    print(f\"Found {len(npz_files)} files to process with Fixed MOV logic.\")\n",
    "    \n",
    "    print(\"Starting Parallel Feature Extraction (Fixed MOV=6)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run Parallel\n",
    "    results = Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(process_file_expanded_fixed)(\n",
    "            f, eeg_ex, ecg_ex, emg_ex, mov1d_ex, mov_triax_ex, n_mov_channels=6\n",
    "        )\n",
    "        for f in npz_files\n",
    "    )\n",
    "    \n",
    "    # Consolidate\n",
    "    X_expanded_list = []\n",
    "    y_expanded_list = []\n",
    "\n",
    "    for X_f, y_f in results:\n",
    "        if X_f is not None and len(X_f) > 0:\n",
    "            X_expanded_list.append(X_f)\n",
    "            y_expanded_list.append(y_f)\n",
    "            \n",
    "    if X_expanded_list:\n",
    "        # Check shapes before concat to be sure\n",
    "        shapes = [x.shape[1] for x in X_expanded_list]\n",
    "        if len(set(shapes)) > 1:\n",
    "            print(f\"CRITICAL WARNING: Found inconsistent feature lengths: {set(shapes)}\")\n",
    "            from collections import Counter\n",
    "            mode_shape = Counter(shapes).most_common(1)[0][0]\n",
    "            print(f\"Filtering to keep only shape {mode_shape}...\")\n",
    "            filtered_X = []\n",
    "            filtered_y = []\n",
    "            for X_f, y_f in zip(X_expanded_list, y_expanded_list):\n",
    "                if X_f.shape[1] == mode_shape:\n",
    "                    filtered_X.append(X_f)\n",
    "                    filtered_y.append(y_f)\n",
    "            X_expanded_list = filtered_X\n",
    "            y_expanded_list = filtered_y\n",
    "\n",
    "        X_expanded = np.concatenate(X_expanded_list, axis=0)\n",
    "        y_expanded = np.concatenate(y_expanded_list, axis=0)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nExtraction Complete in {elapsed:.2f} seconds.\")\n",
    "        print(f\"Final Feature Matrix Shape: {X_expanded.shape}\")\n",
    "        print(f\"Final Label Vector Shape: {y_expanded.shape}\")\n",
    "        \n",
    "        # Save (assuming FEATURE_DIR is defined elsewhere)\n",
    "        out_file = FEATURE_DIR / \"expanded_features_fixed.npz\"\n",
    "        np.savez_compressed(out_file, X_expanded=X_expanded, y_expanded=y_expanded)\n",
    "        print(f\"Saved expanded features to: {out_file}\")\n",
    "    else:\n",
    "        print(\"No features extracted.\")\n",
    "else:\n",
    "    print(f\"Output directory not found: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffa0ef",
   "metadata": {},
   "source": [
    "# Ibrahim's Lighter Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ed0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGFeatureExtractorLight class defined.\n",
      "Total feature count: 28\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "max\n",
      "min\n",
      "range\n",
      "iqr\n",
      "kurtosis\n",
      "skew\n",
      "Q1\n",
      "Q3\n",
      "rms\n",
      "line_length\n",
      "zero_crossings\n",
      "peak_freq\n",
      "weighted_mean_freq\n",
      "median_freq\n",
      "power_delta\n",
      "power_theta\n",
      "power_alpha\n",
      "power_beta\n",
      "rel_power_delta\n",
      "rel_power_theta\n",
      "rel_power_alpha\n",
      "rel_power_beta\n",
      "ratio_theta_alpha\n",
      "ratio_alpha_beta\n",
      "ratio_delta_beta\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "class EEGFeatureExtractorLight:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    # --------- TIME DOMAIN (light) ---------\n",
    "    def extract_time_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        # Basic stats\n",
    "        f['mean'] = float(np.mean(x))\n",
    "        f['std'] = float(np.std(x))\n",
    "        f['median'] = float(np.median(x))\n",
    "        f['max'] = float(np.max(x))\n",
    "        f['min'] = float(np.min(x))\n",
    "        f['range'] = f['max'] - f['min']\n",
    "        f['iqr'] = float(stats.iqr(x))\n",
    "        f['kurtosis'] = float(stats.kurtosis(x))\n",
    "        f['skew'] = float(stats.skew(x))\n",
    "        f['Q1'] = float(np.percentile(x, 25))\n",
    "        f['Q3'] = float(np.percentile(x, 75))\n",
    "\n",
    "        # Temporal activity measures\n",
    "        diff1 = np.diff(x)\n",
    "        f['rms'] = float(np.sqrt(np.mean(x**2)))\n",
    "        f['line_length'] = float(np.sum(np.abs(diff1)))\n",
    "\n",
    "        centered = x - f['mean']\n",
    "        f['zero_crossings'] = float(\n",
    "            np.sum(np.diff(np.signbit(centered).astype(int)) != 0)\n",
    "        )\n",
    "\n",
    "        return f\n",
    "\n",
    "    # --------- FREQUENCY DOMAIN (light) ---------\n",
    "    def extract_frequency_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        freqs, psd = signal.welch(x, self.sfreq, nperseg=min(len(x), 256))\n",
    "        psd_sum = np.sum(psd) + 1e-12\n",
    "        psd_norm = psd / psd_sum\n",
    "\n",
    "        # Basic spectral features\n",
    "        f['peak_freq'] = float(freqs[np.argmax(psd)])\n",
    "        f['weighted_mean_freq'] = float(np.sum(freqs * psd_norm))\n",
    "        f['median_freq'] = float(\n",
    "            freqs[np.where(np.cumsum(psd_norm) >= 0.5)[0][0]]\n",
    "        )\n",
    "\n",
    "        # Band powers (delta, theta, alpha, beta)\n",
    "        bands = {\n",
    "            'delta': (1, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta':  (13, 30),\n",
    "        }\n",
    "\n",
    "        total_power = 0.0\n",
    "        for band, (lo, hi) in bands.items():\n",
    "            idx = (freqs >= lo) & (freqs <= hi)\n",
    "            power = simpson(y=psd[idx], x=freqs[idx]) if np.any(idx) else 0.0\n",
    "            f[f'power_{band}'] = float(power)\n",
    "            total_power += power\n",
    "\n",
    "        total_power = total_power + 1e-12\n",
    "\n",
    "        # Relative band powers\n",
    "        for band in bands.keys():\n",
    "            f[f'rel_power_{band}'] = float(\n",
    "                f[f'power_{band}'] / total_power\n",
    "            )\n",
    "\n",
    "        # A few simple ratios\n",
    "        f['ratio_theta_alpha'] = f['power_theta'] / (f['power_alpha'] + 1e-12)\n",
    "        f['ratio_alpha_beta']  = f['power_alpha'] / (f['power_beta'] + 1e-12)\n",
    "        f['ratio_delta_beta']  = f['power_delta'] / (f['power_beta'] + 1e-12)\n",
    "\n",
    "        return f\n",
    "\n",
    "    # --------- TIME-FREQUENCY (disabled for speed) ---------\n",
    "    def extract_time_frequency_domain(self, x):\n",
    "        # Return empty dict to keep interface compatible\n",
    "        return {}\n",
    "\n",
    "    # --------- DECOMPOSITION (disabled for speed) ---------\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        # Return empty dict to keep interface compatible\n",
    "        return {}\n",
    "\n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_time_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "\n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "    \n",
    "print(\"EEGFeatureExtractorLight class defined.\")\n",
    "\n",
    "extractor = EEGFeatureExtractorLight()\n",
    "dummy = np.random.randn(1000)  # 4 seconds at 250Hz\n",
    "\n",
    "_ = extractor.list_feature_names(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07945e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGFeatureExtractorLight class defined.\n",
      "Total feature count: 23\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "range\n",
      "skewness\n",
      "kurtosis\n",
      "rms\n",
      "max\n",
      "min\n",
      "IQR\n",
      "Q1\n",
      "Q3\n",
      "heart_rate\n",
      "heart_rate_variability\n",
      "rr_range\n",
      "qrs_count\n",
      "dominant_freq\n",
      "total_power\n",
      "norm_peak_band_energy\n",
      "norm_harmonics_energy\n",
      "norm_peak_plus_harmonics_energy\n",
      "dct_mean\n",
      "dct_var\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import scipy.fftpack as fftpack\n",
    "import scipy.stats as stats\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "class ECGFeatureExtractorLight:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    # --- Helper: Pan-Tompkins QRS Detector (Simplified) ---\n",
    "    def _detect_qrs(self, x):\n",
    "        # A. Bandpass Filter (5-15Hz)\n",
    "        sos = signal.butter(3, [5, 15], 'bandpass', fs=self.sfreq, output='sos')\n",
    "        filtered = signal.sosfilt(sos, x)\n",
    "        \n",
    "        # B. Derivative\n",
    "        diff = np.diff(filtered)\n",
    "        \n",
    "        # C. Squaring\n",
    "        squared = diff ** 2\n",
    "        \n",
    "        # D. Moving Window Integration (150ms)\n",
    "        window_size = int(0.15 * self.sfreq)\n",
    "        integrated = np.convolve(squared, np.ones(window_size)/window_size, mode='same')\n",
    "        \n",
    "        # E. Peak Finding\n",
    "        thresh = np.mean(integrated) * 2\n",
    "        peaks, _ = signal.find_peaks(integrated,\n",
    "                                     height=thresh,\n",
    "                                     distance=int(0.2 * self.sfreq))\n",
    "        return peaks\n",
    "\n",
    "    # --- Helper: Normalized energy in peak band & harmonics ---\n",
    "    def _normalized_peak_harmonic_energy(self, freqs, psd, n_harmonics=3, band_width=0.5):\n",
    "        \"\"\"\n",
    "        Compute normalized energy around the dominant frequency and its harmonics.\n",
    "        \"\"\"\n",
    "        freqs = np.asarray(freqs)\n",
    "        psd = np.asarray(psd)\n",
    "\n",
    "        if len(freqs) == 0:\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        total_power = simpson(y=psd, x=freqs)\n",
    "        if total_power <= 0:\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        peak_idx = np.argmax(psd)\n",
    "        f0 = freqs[peak_idx]\n",
    "\n",
    "        energy_fund = 0.0\n",
    "        energy_harm = 0.0\n",
    "\n",
    "        for k in range(1, n_harmonics + 1):\n",
    "            fk = k * f0\n",
    "            if fk - band_width > freqs[-1]:\n",
    "                break\n",
    "\n",
    "            band_mask = (freqs >= fk - band_width) & (freqs <= fk + band_width)\n",
    "            if not np.any(band_mask):\n",
    "                continue\n",
    "\n",
    "            band_energy = simpson(y=psd[band_mask], x=freqs[band_mask])\n",
    "\n",
    "            if k == 1:\n",
    "                energy_fund += band_energy\n",
    "            else:\n",
    "                energy_harm += band_energy\n",
    "\n",
    "        eps = 1e-12\n",
    "        norm_peak = energy_fund / (total_power + eps)\n",
    "        norm_harm = energy_harm / (total_power + eps)\n",
    "        norm_total = (energy_fund + energy_harm) / (total_power + eps)\n",
    "\n",
    "        return float(norm_peak), float(norm_harm), float(norm_total)\n",
    "\n",
    "    # ---------- TIME DOMAIN (light) ----------\n",
    "    def extract_time_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        # Basic Stats\n",
    "        f['mean'] = float(np.mean(x))\n",
    "        f['std'] = float(np.std(x))\n",
    "        f['median'] = float(np.median(x))\n",
    "        f['range'] = float(np.max(x) - np.min(x))\n",
    "        f['skewness'] = float(stats.skew(x))\n",
    "        f['kurtosis'] = float(stats.kurtosis(x))\n",
    "        f['rms'] = float(np.sqrt(np.mean(x**2)))\n",
    "        f['max'] = float(np.max(x))\n",
    "        f['min'] = float(np.min(x))\n",
    "        f['IQR'] = float(stats.iqr(x))\n",
    "        f['Q1'] = float(np.percentile(x, 25))\n",
    "        f['Q3'] = float(np.percentile(x, 75))\n",
    "\n",
    "        # QRS & HR Features\n",
    "        r_peaks = self._detect_qrs(x)\n",
    "\n",
    "        if len(r_peaks) > 1:\n",
    "            rr_intervals = np.diff(r_peaks) / self.sfreq  # seconds\n",
    "            f['heart_rate'] = float(60.0 / np.mean(rr_intervals))      # BPM\n",
    "            f['heart_rate_variability'] = float(np.std(rr_intervals))  # SDNN\n",
    "            f['rr_range'] = float(np.max(rr_intervals) - np.min(rr_intervals))\n",
    "            f['qrs_count'] = int(len(r_peaks))\n",
    "        else:\n",
    "            f['heart_rate'] = 0.0\n",
    "            f['heart_rate_variability'] = 0.0\n",
    "            f['rr_range'] = 0.0\n",
    "            f['qrs_count'] = int(len(r_peaks))\n",
    "\n",
    "        # (LPC removed for speed)\n",
    "\n",
    "        return f\n",
    "\n",
    "    # ---------- FREQUENCY DOMAIN (light) ----------\n",
    "    def extract_frequency_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        freqs, psd = signal.welch(x, self.sfreq, nperseg=min(len(x), 256))\n",
    "\n",
    "        f['dominant_freq'] = float(freqs[np.argmax(psd)])\n",
    "        f['total_power'] = float(np.sum(psd))\n",
    "\n",
    "        # Normalized peak + harmonics energy\n",
    "        norm_peak, norm_harm, norm_total = self._normalized_peak_harmonic_energy(\n",
    "            freqs, psd, n_harmonics=3, band_width=0.5\n",
    "        )\n",
    "        f['norm_peak_band_energy'] = norm_peak\n",
    "        f['norm_harmonics_energy'] = norm_harm\n",
    "        f['norm_peak_plus_harmonics_energy'] = norm_total\n",
    "\n",
    "        # Simple DCT summary (cheap)\n",
    "        dct_val = fftpack.dct(x, type=2, norm='ortho')\n",
    "        f['dct_mean'] = float(np.mean(np.abs(dct_val)))\n",
    "        f['dct_var'] = float(np.var(dct_val))\n",
    "\n",
    "        # (MFCC + Hilbert removed for speed)\n",
    "\n",
    "        return f\n",
    "\n",
    "    # ---------- TIME-FREQUENCY (disabled) ----------\n",
    "    def extract_time_frequency_domain(self, x):\n",
    "        # Empty dict to keep interface compatible\n",
    "        return {}\n",
    "\n",
    "    # ---------- DECOMPOSITION (disabled) ----------\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        # Empty dict to keep interface compatible\n",
    "        return {}\n",
    "\n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_time_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "\n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "print(\"ECGFeatureExtractorLight class defined.\")\n",
    "\n",
    "extractor = ECGFeatureExtractorLight()\n",
    "dummy = np.random.randn(1000)  # 4 seconds at 250Hz\n",
    "\n",
    "_ = extractor.list_feature_names(dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bfc80b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMGFeatureExtractorLight class defined.\n",
      "Total feature count: 36\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "max\n",
      "min\n",
      "range\n",
      "iqr\n",
      "kurtosis\n",
      "skewness\n",
      "Q1\n",
      "Q3\n",
      "integrated_EMG\n",
      "mean_absolute_value\n",
      "simple_square_integral\n",
      "RMS\n",
      "variance\n",
      "waveform_length\n",
      "difference_absolute_mean_value\n",
      "difference_variance\n",
      "difference_absolute_standard_deviation\n",
      "integrated_absolute_second_derivative\n",
      "integrated_absolute_third_derivative\n",
      "second_order_moment\n",
      "Willison_amplitude\n",
      "myopulse_percentage_rate\n",
      "zero_crossings\n",
      "slope_sign_changes\n",
      "Hjorth_activity\n",
      "Hjorth_mobility\n",
      "Hjorth_complexity\n",
      "integrated_exponential\n",
      "integrated_absolute_log\n",
      "FFT_mean_mag\n",
      "FFT_total_energy\n",
      "FFT_dominant_freq\n",
      "FFT_spectral_centroid\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "\n",
    "class EMGFeatureExtractorLight:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    # ---------- TIME DOMAIN (kept, slightly cleaned) ----------\n",
    "    def extract_time_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        # --- 1. Basic Statistics ---\n",
    "        f['mean'] = float(np.mean(x))\n",
    "        f['std'] = float(np.std(x))\n",
    "        f['median'] = float(np.median(x))\n",
    "        f['max'] = float(np.max(x))\n",
    "        f['min'] = float(np.min(x))\n",
    "        f['range'] = f['max'] - f['min']\n",
    "        f['iqr'] = float(stats.iqr(x))\n",
    "        f['kurtosis'] = float(stats.kurtosis(x))\n",
    "        f['skewness'] = float(stats.skew(x))\n",
    "        f['Q1'] = float(np.percentile(x, 25))\n",
    "        f['Q3'] = float(np.percentile(x, 75))\n",
    "\n",
    "        # --- 2. Conventional EMG Features ---\n",
    "        diff1 = np.diff(x)\n",
    "\n",
    "        # Integrated EMG (IEMG)\n",
    "        f['integrated_EMG'] = float(np.sum(np.abs(x)))\n",
    "\n",
    "        # Mean Absolute Value (MAV)\n",
    "        f['mean_absolute_value'] = float(np.mean(np.abs(x)))\n",
    "\n",
    "        # Simple Square Integral (SSI)\n",
    "        f['simple_square_integral'] = float(np.sum(x**2))\n",
    "\n",
    "        # Root Mean Square (RMS)\n",
    "        f['RMS'] = float(np.sqrt(np.mean(x**2)))\n",
    "\n",
    "        # Variance\n",
    "        f['variance'] = float(np.var(x))\n",
    "\n",
    "        # Waveform Length (WL)\n",
    "        f['waveform_length'] = float(np.sum(np.abs(diff1)))\n",
    "\n",
    "        # Difference Absolute Mean Value (DAMV)\n",
    "        f['difference_absolute_mean_value'] = float(np.mean(np.abs(diff1)))\n",
    "\n",
    "        # Difference Variance\n",
    "        f['difference_variance'] = float(np.var(diff1))\n",
    "\n",
    "        # Difference Absolute Standard Deviation (DASD)\n",
    "        f['difference_absolute_standard_deviation'] = float(np.std(diff1))\n",
    "\n",
    "        # Higher-order difference integrals (still cheap)\n",
    "        diff2 = np.diff(diff1)\n",
    "        diff3 = np.diff(diff2)\n",
    "        f['integrated_absolute_second_derivative'] = float(np.sum(np.abs(diff2)))\n",
    "        f['integrated_absolute_third_derivative'] = float(np.sum(np.abs(diff3)))\n",
    "\n",
    "        # Second Order Moment\n",
    "        f['second_order_moment'] = float(np.mean(x**2))\n",
    "\n",
    "        # Willison Amplitude (WAMP)\n",
    "        threshold = 0.1 * f['std']\n",
    "        f['Willison_amplitude'] = int(np.sum(np.abs(diff1) > threshold))\n",
    "\n",
    "        # Myopulse Percentage Rate (MYOP)\n",
    "        f['myopulse_percentage_rate'] = float(\n",
    "            np.sum(np.abs(x) > threshold) / len(x)\n",
    "        )\n",
    "\n",
    "        # --- 3. Counts / Changes ---\n",
    "        centered = x - f['mean']\n",
    "        f['zero_crossings'] = int(\n",
    "            np.sum(np.diff(np.signbit(centered).astype(int)) != 0)\n",
    "        )\n",
    "\n",
    "        f['slope_sign_changes'] = int(\n",
    "            np.sum(np.diff(np.sign(diff1)) != 0)\n",
    "        )\n",
    "\n",
    "        # --- 4. Hjorth Parameters ---\n",
    "        f['Hjorth_activity'] = f['variance']\n",
    "\n",
    "        if f['variance'] > 0:\n",
    "            var_diff1 = float(np.var(diff1))\n",
    "            f['Hjorth_mobility'] = float(np.sqrt(var_diff1 / f['variance']))\n",
    "        else:\n",
    "            var_diff1 = 0.0\n",
    "            f['Hjorth_mobility'] = 0.0\n",
    "\n",
    "        if f['Hjorth_mobility'] > 0 and var_diff1 > 0:\n",
    "            var_diff2 = float(np.var(diff2))\n",
    "            mob_diff = np.sqrt(var_diff2 / var_diff1)\n",
    "            f['Hjorth_complexity'] = float(mob_diff / f['Hjorth_mobility'])\n",
    "        else:\n",
    "            f['Hjorth_complexity'] = 0.0\n",
    "\n",
    "        # --- 5. Safe exponential / log integrals (clipped) ---\n",
    "        try:\n",
    "            f['integrated_exponential'] = float(\n",
    "                np.sum(np.exp(np.clip(x, -5, 5)))\n",
    "            )\n",
    "        except Exception:\n",
    "            f['integrated_exponential'] = 0.0\n",
    "\n",
    "        f['integrated_absolute_log'] = float(\n",
    "            np.sum(np.log(np.abs(x) + 1e-6))\n",
    "        )\n",
    "\n",
    "        return f\n",
    "\n",
    "    # ---------- FREQUENCY DOMAIN (very light) ----------\n",
    "    def extract_frequency_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        # Simple FFT-based features\n",
    "        fft_vals = np.fft.rfft(x)\n",
    "        freqs = np.fft.rfftfreq(len(x), d=1.0 / self.sfreq)\n",
    "        mag = np.abs(fft_vals)\n",
    "\n",
    "        if len(mag) > 0:\n",
    "            mag_sum = np.sum(mag) + 1e-12\n",
    "            f['FFT_mean_mag'] = float(np.mean(mag))\n",
    "            f['FFT_total_energy'] = float(np.sum(mag**2))\n",
    "\n",
    "            # dominant frequency\n",
    "            f['FFT_dominant_freq'] = float(freqs[np.argmax(mag)])\n",
    "\n",
    "            # spectral centroid\n",
    "            f['FFT_spectral_centroid'] = float(np.sum(freqs * mag) / mag_sum)\n",
    "        else:\n",
    "            f['FFT_mean_mag'] = 0.0\n",
    "            f['FFT_total_energy'] = 0.0\n",
    "            f['FFT_dominant_freq'] = 0.0\n",
    "            f['FFT_spectral_centroid'] = 0.0\n",
    "\n",
    "        # (AR models removed for speed)\n",
    "        return f\n",
    "\n",
    "    # ---------- DECOMPOSITION (disabled) ----------\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        # Skip wavelet features to save time\n",
    "        return {}\n",
    "\n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "\n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "\n",
    "print(\"EMGFeatureExtractorLight class defined.\")\n",
    "\n",
    "extractor = EMGFeatureExtractorLight(sfreq=250)\n",
    "dummy = np.random.randn(1000)  # 4 seconds @ 250 Hz\n",
    "_ = extractor.list_feature_names(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5c2cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVFeatureExtractorLight classes defined.\n",
      "\n",
      "--- TRIAXIAL FEATURES ---\n",
      "Total feature count (triax): 15\n",
      "Feature names:\n",
      "triax_mean_X\n",
      "triax_mean_Y\n",
      "triax_mean_Z\n",
      "triax_variance_X\n",
      "triax_variance_Y\n",
      "triax_variance_Z\n",
      "triax_total_sum_vector\n",
      "triax_sum_vector_magnitude\n",
      "triax_activity_single_magnitude_area\n",
      "triax_dynamic_sum_vector\n",
      "triax_vertical_acceleration\n",
      "triax_angular_velocity_aggregate\n",
      "triax_fft_mean_mag\n",
      "triax_fft_energy\n",
      "triax_dominant_freq\n",
      "\n",
      "--- 1D FEATURES ---\n",
      "Total feature count (1D): 24\n",
      "Feature names:\n",
      "mean\n",
      "std\n",
      "median\n",
      "max\n",
      "min\n",
      "range\n",
      "iqr\n",
      "kurtosis\n",
      "skewness\n",
      "Q1\n",
      "Q3\n",
      "signal_magnitude_area\n",
      "zero_crossing_rate\n",
      "peak_count\n",
      "val_first_minus_last\n",
      "val_first_minus_max\n",
      "val_first_minus_min\n",
      "val_last_minus_max\n",
      "val_last_minus_min\n",
      "rank_last_value\n",
      "rank_min_value\n",
      "fft_mean_mag\n",
      "fft_energy\n",
      "dominant_freq\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "\n",
    "class MOVTriaxFeatureExtractorLight:\n",
    "    def __init__(self, sfreq=250, vertical_axis=2):\n",
    "        \"\"\"\n",
    "        Triaxial MOV feature extractor (lightweight).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sfreq : float\n",
    "            Sampling frequency (Hz).\n",
    "        vertical_axis : int\n",
    "            Index of the vertical acceleration axis (0, 1, or 2).\n",
    "        \"\"\"\n",
    "        self.sfreq = sfreq\n",
    "        self.vertical_axis = vertical_axis\n",
    "\n",
    "    # --- Helper: ensure (N, 3) shape ---\n",
    "    def _ensure_triax_shape(self, arr, name=\"acc\"):\n",
    "        \"\"\"\n",
    "        Accepts (N, 3) or (3, N) and returns (N, 3).\n",
    "        \"\"\"\n",
    "        arr = np.asarray(arr)\n",
    "        if arr.ndim != 2 or 3 not in arr.shape:\n",
    "            raise ValueError(f\"{name} must be 2D with one dimension = 3, got shape {arr.shape}\")\n",
    "        if arr.shape[1] == 3:\n",
    "            return arr\n",
    "        else:\n",
    "            # assume (3, N)\n",
    "            return arr.T\n",
    "\n",
    "    # ---------- TIME DOMAIN ----------\n",
    "    def extract_time_domain(self, acc, gyro=None):\n",
    "        \"\"\"\n",
    "        acc : array-like, shape (N, 3) or (3, N)\n",
    "        gyro : array-like, shape (N, 3) or (3, N), optional\n",
    "        \"\"\"\n",
    "        f = {}\n",
    "\n",
    "        acc = self._ensure_triax_shape(acc, name=\"acc\")\n",
    "        ax, ay, az = acc[:, 0], acc[:, 1], acc[:, 2]\n",
    "\n",
    "        # --- Axis-wise stats ---\n",
    "        f['triax_mean_X'] = float(np.mean(ax))\n",
    "        f['triax_mean_Y'] = float(np.mean(ay))\n",
    "        f['triax_mean_Z'] = float(np.mean(az))\n",
    "\n",
    "        f['triax_variance_X'] = float(np.var(ax))\n",
    "        f['triax_variance_Y'] = float(np.var(ay))\n",
    "        f['triax_variance_Z'] = float(np.var(az))\n",
    "\n",
    "        # --- Resultant magnitude ---\n",
    "        mag = np.sqrt(ax**2 + ay**2 + az**2)\n",
    "\n",
    "        # Total sum of vector magnitude over the window\n",
    "        f['triax_total_sum_vector'] = float(np.sum(mag))\n",
    "\n",
    "        # Mean magnitude (average length of the sum vector)\n",
    "        f['triax_sum_vector_magnitude'] = float(np.mean(mag))\n",
    "\n",
    "        # Activity / single magnitude area: sum of |ax|+|ay|+|az|\n",
    "        f['triax_activity_single_magnitude_area'] = float(\n",
    "            np.sum(np.abs(ax) + np.abs(ay) + np.abs(az))\n",
    "        )\n",
    "\n",
    "        # --- Dynamic component (subtract DC/gravity per axis) ---\n",
    "        ax_d = ax - np.mean(ax)\n",
    "        ay_d = ay - np.mean(ay)\n",
    "        az_d = az - np.mean(az)\n",
    "        mag_dyn = np.sqrt(ax_d**2 + ay_d**2 + az_d**2)\n",
    "        f['triax_dynamic_sum_vector'] = float(np.sum(mag_dyn))\n",
    "\n",
    "        # --- Vertical acceleration ---\n",
    "        if self.vertical_axis not in (0, 1, 2):\n",
    "            raise ValueError(\"vertical_axis must be 0, 1, or 2\")\n",
    "        a_vert = acc[:, self.vertical_axis]\n",
    "        f['triax_vertical_acceleration'] = float(np.mean(np.abs(a_vert)))\n",
    "\n",
    "        # --- Angular velocity aggregate (if gyro provided) ---\n",
    "        if gyro is not None:\n",
    "            gyro = self._ensure_triax_shape(gyro, name=\"gyro\")\n",
    "            gx, gy, gz = gyro[:, 0], gyro[:, 1], gyro[:, 2]\n",
    "            gyro_mag = np.sqrt(gx**2 + gy**2 + gz**2)\n",
    "            f['triax_angular_velocity_aggregate'] = float(np.mean(gyro_mag))\n",
    "        else:\n",
    "            f['triax_angular_velocity_aggregate'] = 0.0\n",
    "\n",
    "        return f\n",
    "\n",
    "    # ---------- FREQUENCY DOMAIN ----------\n",
    "    def extract_frequency_domain(self, acc):\n",
    "        \"\"\"\n",
    "        Basic frequency-domain features from resultant magnitude.\n",
    "        \"\"\"\n",
    "        f = {}\n",
    "\n",
    "        acc = self._ensure_triax_shape(acc, name=\"acc\")\n",
    "        ax, ay, az = acc[:, 0], acc[:, 1], acc[:, 2]\n",
    "        mag = np.sqrt(ax**2 + ay**2 + az**2)\n",
    "\n",
    "        fft_vals = np.fft.rfft(mag)\n",
    "        freqs = np.fft.rfftfreq(len(mag), d=1/self.sfreq)\n",
    "        mag_abs = np.abs(fft_vals)\n",
    "\n",
    "        if len(mag_abs) > 0:\n",
    "            f['triax_fft_mean_mag'] = float(np.mean(mag_abs))\n",
    "            f['triax_fft_energy'] = float(np.sum(mag_abs**2))\n",
    "            f['triax_dominant_freq'] = float(freqs[np.argmax(mag_abs)])\n",
    "        else:\n",
    "            f['triax_fft_mean_mag'] = 0.0\n",
    "            f['triax_fft_energy'] = 0.0\n",
    "            f['triax_dominant_freq'] = 0.0\n",
    "\n",
    "        return f\n",
    "\n",
    "    # ---------- DECOMPOSITION DOMAIN (disabled for speed) ----------\n",
    "    def extract_decomposition_domain(self, acc):\n",
    "        # Skip wavelet features for the fast run\n",
    "        return {}\n",
    "\n",
    "    def list_feature_names(self, acc):\n",
    "        all_features = {}\n",
    "        # for triax, we need to pass gyro if you care; here we just ignore gyro\n",
    "        acc = self._ensure_triax_shape(acc, name=\"acc\")\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            # extract_time_domain expects (acc, gyro=None)\n",
    "            if method is self.extract_time_domain:\n",
    "                all_features.update(method(acc))\n",
    "            else:\n",
    "                all_features.update(method(acc))\n",
    "\n",
    "        print(\"Total feature count (triax):\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "\n",
    "class MOV1DFeatureExtractorLight:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    def extract_time_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        # --- 1. Basic Statistics ---\n",
    "        f['mean'] = float(np.mean(x))\n",
    "        f['std'] = float(np.std(x))\n",
    "        f['median'] = float(np.median(x))\n",
    "        f['max'] = float(np.max(x))\n",
    "        f['min'] = float(np.min(x))\n",
    "        f['range'] = f['max'] - f['min']\n",
    "        f['iqr'] = float(stats.iqr(x))\n",
    "        f['kurtosis'] = float(stats.kurtosis(x))\n",
    "        f['skewness'] = float(stats.skew(x))\n",
    "        f['Q1'] = float(np.percentile(x, 25))\n",
    "        f['Q3'] = float(np.percentile(x, 75))\n",
    "\n",
    "        # --- 2. Temporal Features ---\n",
    "        f['signal_magnitude_area'] = float(np.sum(np.abs(x)))\n",
    "\n",
    "        centered = x - f['mean']\n",
    "        f['zero_crossing_rate'] = float(\n",
    "            np.sum(np.diff(np.signbit(centered).astype(int)) != 0) / len(x)\n",
    "        )\n",
    "\n",
    "        peaks, _ = signal.find_peaks(x, prominence=0.5 * f['std'])\n",
    "        f['peak_count'] = int(len(peaks))\n",
    "\n",
    "        # --- 3. Trajectory Features ---\n",
    "        first = x[0]\n",
    "        last = x[-1]\n",
    "        mx = f['max']\n",
    "        mn = f['min']\n",
    "\n",
    "        f['val_first_minus_last'] = float(first - last)\n",
    "        f['val_first_minus_max'] = float(first - mx)\n",
    "        f['val_first_minus_min'] = float(first - mn)\n",
    "        f['val_last_minus_max'] = float(last - mx)\n",
    "        f['val_last_minus_min'] = float(last - mn)\n",
    "\n",
    "        # --- 4. Acceleration Order (Ranking) ---\n",
    "        sorted_x = np.sort(x)\n",
    "        f['rank_last_value'] = float(np.searchsorted(sorted_x, last) / len(x))\n",
    "        f['rank_min_value'] = 0.0\n",
    "\n",
    "        return f\n",
    "\n",
    "    def extract_frequency_domain(self, x):\n",
    "        x = np.asarray(x)\n",
    "        f = {}\n",
    "\n",
    "        fft_vals = np.fft.rfft(x)\n",
    "        freqs = np.fft.rfftfreq(len(x), d=1/self.sfreq)\n",
    "        mag = np.abs(fft_vals)\n",
    "\n",
    "        if len(mag) > 0:\n",
    "            f['fft_mean_mag'] = float(np.mean(mag))\n",
    "            f['fft_energy'] = float(np.sum(mag**2))\n",
    "            f['dominant_freq'] = float(freqs[np.argmax(mag)])\n",
    "        else:\n",
    "            f['fft_mean_mag'] = 0.0\n",
    "            f['fft_energy'] = 0.0\n",
    "            f['dominant_freq'] = 0.0\n",
    "\n",
    "        return f\n",
    "\n",
    "    def extract_decomposition_domain(self, x):\n",
    "        # Skip wavelet features for speed\n",
    "        return {}\n",
    "\n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "            self.extract_decomposition_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "\n",
    "        print(\"Total feature count (1D):\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "\n",
    "print(\"MOVFeatureExtractorLight classes defined.\")\n",
    "\n",
    "# ---- Dummy test for MOVTriaxFeatureExtractor ----\n",
    "\n",
    "# Create dummy triaxial accelerometer data: 1000 samples of (x,y,z)\n",
    "np.random.seed(0)\n",
    "acc_dummy = np.random.randn(1000, 3)  # shape (1000, 3)\n",
    "\n",
    "# Create dummy gyro data (optional)\n",
    "gyro_dummy = np.random.randn(1000, 3)\n",
    "\n",
    "# Instantiate extractor\n",
    "triax_extractor = MOVTriaxFeatureExtractorLight(sfreq=250, vertical_axis=2)\n",
    "\n",
    "print(\"\\n--- TRIAXIAL FEATURES ---\")\n",
    "_ = triax_extractor.list_feature_names(acc_dummy)\n",
    "\n",
    "\n",
    "# ---- Dummy test for MOV1DFeatureExtractor ----\n",
    "\n",
    "# Create dummy 1D motion signal\n",
    "x_dummy = np.random.randn(1000)\n",
    "\n",
    "# Instantiate extractor\n",
    "mov1d_extractor = MOV1DFeatureExtractorLight(sfreq=250)\n",
    "\n",
    "print(\"\\n--- 1D FEATURES ---\")\n",
    "_ = mov1d_extractor.list_feature_names(x_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c822579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark on 3 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark Results ---\n",
      "Files Processed: 3\n",
      "Total Windows in Subset: 1144\n",
      "Time Taken: 26.18 seconds\n",
      "Avg Time per Window: 0.0229 seconds\n",
      "\n",
      "--- Projection ---\n",
      "Estimated Time for 562817 windows: 3.58 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:   26.1s finished\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Ensure OUTPUT_DIR is defined\n",
    "OUTPUT_DIR = Path(\"F:\\Rice\\Rice F25\\Seizure Project\\Processed_Gemini_V5_Augmented\")  # Update as needed\n",
    "BASE_PATH = Path(\"F:\\Rice\\Rice F25\\Seizure Project\")\n",
    "FEATURE_DIR = BASE_PATH / 'Features'\n",
    "FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_file_expanded_light(\n",
    "    npz_file,\n",
    "    eeg_ext,\n",
    "    ecg_ext,\n",
    "    emg_ext,\n",
    "    mov1d_ext,\n",
    "    mov_triax_ext,\n",
    "    n_mov_channels=6,\n",
    "    bg_per_sz=2,\n",
    "    max_bg_if_no_sz=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads an NPZ file, extracts features with FIXED MOV channel count,\n",
    "    and performs LIGHT subsampling of windows:\n",
    "\n",
    "      - Keep ALL seizure windows (y == 1)\n",
    "      - Add up to `bg_per_sz` background windows per seizure\n",
    "      - If no seizures in file, keep up to `max_bg_if_no_sz` background windows\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    npz_file : Path-like\n",
    "        .npz file containing X_teacher, X_student_bio, X_student_mov, y\n",
    "    eeg_ext, ecg_ext, emg_ext, mov1d_ext, mov_triax_ext :\n",
    "        Feature extractor instances with extract_* methods.\n",
    "    n_mov_channels : int\n",
    "        Expected number of MOV channels (1D) per window (usually 6).\n",
    "    bg_per_sz : int\n",
    "        Number of background windows to sample per seizure window.\n",
    "    max_bg_if_no_sz : int\n",
    "        Max number of background windows to keep if no seizures present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    file_features : np.ndarray of shape (n_kept_windows, n_features)\n",
    "    y_kept        : np.ndarray of shape (n_kept_windows,)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = np.load(npz_file)\n",
    "        X_t     = data['X_teacher']       # (n_win, n_eeg_ch, n_samples_eeg)\n",
    "        X_s_bio = data['X_student_bio']   # (n_win, 2, n_samples_bio) -> ECG, EMG\n",
    "        X_s_mov = data['X_student_mov']   # (n_win, 6, n_samples_mov) -> 3 Acc, 3 Gyro\n",
    "        y       = data['y']               # (n_win,)\n",
    "\n",
    "        n_wins = X_t.shape[0]\n",
    "\n",
    "        # ---------- 1. Choose which windows to keep ----------\n",
    "        idx_all = np.arange(n_wins)\n",
    "        idx_sz  = idx_all[y == 1]\n",
    "        idx_bg  = idx_all[y == 0]\n",
    "\n",
    "        keep_idx = []\n",
    "\n",
    "        if len(idx_sz) > 0:\n",
    "            # Keep all seizure windows\n",
    "            keep_idx.extend(idx_sz.tolist())\n",
    "\n",
    "            # Sample background windows relative to number of seizures\n",
    "            n_bg_target = min(len(idx_bg), bg_per_sz * len(idx_sz))\n",
    "            if n_bg_target > 0:\n",
    "                bg_sample = np.random.choice(idx_bg, size=n_bg_target, replace=False)\n",
    "                keep_idx.extend(bg_sample.tolist())\n",
    "        else:\n",
    "            # No seizures in this file: keep a capped number of background windows\n",
    "            n_bg_target = min(len(idx_bg), max_bg_if_no_sz)\n",
    "            if n_bg_target > 0:\n",
    "                bg_sample = np.random.choice(idx_bg, size=n_bg_target, replace=False)\n",
    "                keep_idx.extend(bg_sample.tolist())\n",
    "\n",
    "        # If nothing to keep, skip this file\n",
    "        if not keep_idx:\n",
    "            return None, None\n",
    "\n",
    "        keep_idx = np.array(keep_idx, dtype=int)\n",
    "        # Shuffle so seizures and backgrounds are mixed\n",
    "        np.random.shuffle(keep_idx)\n",
    "\n",
    "        # ---------- 2. Feature extraction on selected windows ----------\n",
    "        file_features = []\n",
    "\n",
    "        for i in keep_idx:\n",
    "            win_feats = []\n",
    "\n",
    "            # --- 1. EEG Features (Teacher) ---\n",
    "            # X_t[i] shape: (n_eeg_ch, n_samples_eeg)\n",
    "            for ch_idx in range(X_t.shape[1]):\n",
    "                win_feats.append(extract_channel_features(eeg_ext, X_t[i, ch_idx]))\n",
    "\n",
    "            # --- 2. Student Bio Features ---\n",
    "            # Index 0: ECG\n",
    "            # Index 1: EMG\n",
    "            if X_s_bio.shape[1] > 0:\n",
    "                # ECG\n",
    "                win_feats.append(extract_channel_features(ecg_ext, X_s_bio[i, 0]))\n",
    "\n",
    "            if X_s_bio.shape[1] > 1:\n",
    "                # EMG\n",
    "                win_feats.append(extract_channel_features(emg_ext, X_s_bio[i, 1]))\n",
    "\n",
    "            # --- 3. MOV 1D Features (per channel) ---\n",
    "            # X_s_mov shape: (n_win, 6, n_samples_mov)\n",
    "            # Channels 0-2: Accel X,Y,Z\n",
    "            # Channels 3-5: Gyro X,Y,Z\n",
    "            for k in range(n_mov_channels):\n",
    "                if k < X_s_mov.shape[1]:\n",
    "                    sig = X_s_mov[i, k]\n",
    "                else:\n",
    "                    # Zero padding if fewer channels than expected\n",
    "                    sig = np.zeros(X_s_mov.shape[2], dtype=float)\n",
    "\n",
    "                feat_vec = extract_channel_features(mov1d_ext, sig)\n",
    "                win_feats.append(feat_vec)\n",
    "\n",
    "            # --- 4. MOV Triax Features ---\n",
    "            # Accel (first 3 channels)\n",
    "            n_mov_samples = X_s_mov.shape[2]\n",
    "            acc_3axis = np.zeros((n_mov_samples, 3), dtype=float)\n",
    "            if X_s_mov.shape[1] >= 3:\n",
    "                acc_3axis = X_s_mov[i, 0:3].T  # (N, 3)\n",
    "\n",
    "            # Gyro (next 3 channels)\n",
    "            gyro_3axis = np.zeros((n_mov_samples, 3), dtype=float)\n",
    "            if X_s_mov.shape[1] >= 6:\n",
    "                gyro_3axis = X_s_mov[i, 3:6].T  # (N, 3)\n",
    "\n",
    "            triax_feats = extract_triax_mov_features(mov_triax_ext, acc_3axis, gyro_3axis)\n",
    "            win_feats.append(triax_feats)\n",
    "\n",
    "            # Concatenate all feature vectors for this window\n",
    "            file_features.append(np.concatenate(win_feats))\n",
    "\n",
    "        file_features = np.array(file_features)\n",
    "        y_kept = y[keep_idx]\n",
    "\n",
    "        return file_features, y_kept\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {getattr(npz_file, 'name', npz_file)}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the list of files is available\n",
    "if 'npz_files' not in locals():\n",
    "    npz_files = sorted(list(OUTPUT_DIR.glob(\"*.npz\")))\n",
    "\n",
    "# Select a subset of 3 files\n",
    "subset_files = npz_files[:3]\n",
    "print(f\"Running benchmark on {len(subset_files)} files...\")\n",
    "\n",
    "start_time_subset = time.time()\n",
    "\n",
    "# Run extraction on the subset\n",
    "results_subset = Parallel(n_jobs=4, verbose=1)(\n",
    "    delayed(process_file_expanded_light)(\n",
    "        f, eeg_ex, ecg_ex, emg_ex, mov1d_ex, mov_triax_ex, n_mov_channels=6\n",
    "    )\n",
    "    for f in subset_files\n",
    ")\n",
    "\n",
    "end_time_subset = time.time()\n",
    "elapsed_subset = end_time_subset - start_time_subset\n",
    "\n",
    "# Calculate total windows processed\n",
    "total_windows_subset = 0\n",
    "valid_files_count = 0\n",
    "\n",
    "for X_f, y_f in results_subset:\n",
    "    if X_f is not None:\n",
    "        total_windows_subset += X_f.shape[0]\n",
    "        valid_files_count += 1\n",
    "\n",
    "if total_windows_subset > 0:\n",
    "    time_per_window = elapsed_subset / total_windows_subset\n",
    "    \n",
    "    target_total_windows = 562817\n",
    "    estimated_total_seconds = time_per_window * target_total_windows\n",
    "    estimated_hours = estimated_total_seconds / 3600\n",
    "    \n",
    "    print(f\"\\n--- Benchmark Results ---\")\n",
    "    print(f\"Files Processed: {valid_files_count}\")\n",
    "    print(f\"Total Windows in Subset: {total_windows_subset}\")\n",
    "    print(f\"Time Taken: {elapsed_subset:.2f} seconds\")\n",
    "    print(f\"Avg Time per Window: {time_per_window:.4f} seconds\")\n",
    "    print(f\"\\n--- Projection ---\")\n",
    "    print(f\"Estimated Time for {target_total_windows} windows: {estimated_hours:.2f} hours\")\n",
    "else:\n",
    "    print(\"Benchmark failed: No windows extracted from the subset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7f2e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest with 2699 files to process.\n",
      "Initializing extractors...\n",
      "Starting parallel extraction on 2699 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=4)]: Done 874 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=4)]: Done 1144 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=4)]: Done 1544 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=4)]: Done 1928 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=4)]: Done 2306 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=4)]: Done 2692 out of 2699 | elapsed: 26.8min remaining:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done 2699 out of 2699 | elapsed: 26.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction finished in 1612.71 seconds.\n",
      "Aggregating results...\n",
      "Final Feature Matrix Shape: (562729, 274)\n",
      "Final Label Vector Shape: (562729,)\n",
      "Saved optimized features to F:\\Rice\\Rice F25\\Seizure Project\\Features\\lightweight_features.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "# Define paths\n",
    "OUTPUT_DIR = Path(\"F:\\Rice\\Rice F25\\Seizure Project\\Processed_Gemini_V5_Augmented\")  # Update as needed\n",
    "BASE_PATH = Path(\"F:\\Rice\\Rice F25\\Seizure Project\")\n",
    "DATA_DIR = BASE_PATH / \"Processed_Gemini_V5_Augmented\"\n",
    "FEATURE_DIR = BASE_PATH / \"Features\"\n",
    "FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MANIFEST_FILE = FEATURE_DIR / \"processing_manifest.pkl\"\n",
    "OUTPUT_FILE = FEATURE_DIR / \"lightweight_features.npz\"\n",
    "\n",
    "# Load Manifest\n",
    "try:\n",
    "    with open(MANIFEST_FILE, 'rb') as f:\n",
    "        manifest = pickle.load(f)\n",
    "    print(f\"Loaded manifest with {len(manifest)} files to process.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Manifest file {MANIFEST_FILE} not found.\")\n",
    "    manifest = {}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def extract_channel_features(extractor, signal_data):\n",
    "    \"\"\"Runs all extraction methods on a single 1D channel.\"\"\"\n",
    "    feats = {}\n",
    "    try:\n",
    "        feats.update(extractor.extract_time_domain(signal_data))\n",
    "        feats.update(extractor.extract_frequency_domain(signal_data))\n",
    "        if hasattr(extractor, 'extract_time_frequency_domain'):\n",
    "            feats.update(extractor.extract_time_frequency_domain(signal_data))\n",
    "        if hasattr(extractor, 'extract_decomposition_domain'):\n",
    "            feats.update(extractor.extract_decomposition_domain(signal_data))\n",
    "        # Return sorted values for consistency\n",
    "        return np.array([feats[k] for k in sorted(feats.keys())])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_triax_mov_features(extractor, acc, gyro=None):\n",
    "    \"\"\"Runs extraction on triaxial MOV data.\"\"\"\n",
    "    feats = {}\n",
    "    try:\n",
    "        feats.update(extractor.extract_time_domain(acc, gyro=gyro))\n",
    "        feats.update(extractor.extract_frequency_domain(acc))\n",
    "        feats.update(extractor.extract_decomposition_domain(acc))\n",
    "        return np.array([feats[k] for k in sorted(feats.keys())])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def process_file_manifest(filename, indices, extractors, data_dir, n_mov_channels=6):\n",
    "    \"\"\"Worker function to process specific indices of a file.\"\"\"\n",
    "    eeg_ex, ecg_ex, emg_ex, mov1d_ex, mov_triax_ex = extractors\n",
    "    file_path = data_dir / filename\n",
    "    \n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            # Load and subset data\n",
    "            X_t     = data['X_teacher'][indices]        # (n, 2, 500)\n",
    "            X_s_bio = data['X_student_bio'][indices]    # (n, 2, 500)\n",
    "            X_s_mov = data['X_student_mov'][indices]    # (n, 6, 50)\n",
    "            y       = data['y'][indices]                # (n,)\n",
    "\n",
    "        n_wins = len(y)\n",
    "        file_feats  = []\n",
    "        file_labels = []     # <--- new\n",
    "\n",
    "        for i in range(n_wins):\n",
    "            win_feats = []\n",
    "            valid_window = True\n",
    "\n",
    "            # 1. EEG (Teacher) - 2 channels\n",
    "            for ch in range(X_t.shape[1]):\n",
    "                f = extract_channel_features(eeg_ex, X_t[i, ch])\n",
    "                if f is None:\n",
    "                    valid_window = False\n",
    "                    break\n",
    "                win_feats.append(f)\n",
    "            if not valid_window:\n",
    "                continue\n",
    "\n",
    "            # 2. ECG (Student Bio ch 0)\n",
    "            f_ecg = extract_channel_features(ecg_ex, X_s_bio[i, 0])\n",
    "            if f_ecg is None:\n",
    "                continue\n",
    "            win_feats.append(f_ecg)\n",
    "\n",
    "            # 3. EMG (Student Bio ch 1)\n",
    "            f_emg = extract_channel_features(emg_ex, X_s_bio[i, 1])\n",
    "            if f_emg is None:\n",
    "                continue\n",
    "            win_feats.append(f_emg)\n",
    "\n",
    "            # 4. MOV 1D (Student Mov ch 0-5)\n",
    "            for k in range(n_mov_channels):\n",
    "                if k < X_s_mov.shape[1]:\n",
    "                    sig = X_s_mov[i, k]\n",
    "                else:\n",
    "                    sig = np.zeros(X_s_mov.shape[2], dtype=float)\n",
    "\n",
    "                f_mov = extract_channel_features(mov1d_ex, sig)\n",
    "                if f_mov is None:\n",
    "                    valid_window = False\n",
    "                    break\n",
    "                win_feats.append(f_mov)\n",
    "\n",
    "            if not valid_window:\n",
    "                continue\n",
    "\n",
    "            # 5. MOV Triax\n",
    "            acc_3 = X_s_mov[i, 0:3].T if X_s_mov.shape[1] >= 3 else np.zeros((X_s_mov.shape[2], 3))\n",
    "            gyro_3 = X_s_mov[i, 3:6].T if X_s_mov.shape[1] >= 6 else np.zeros((X_s_mov.shape[2], 3))\n",
    "\n",
    "            f_triax = extract_triax_mov_features(mov_triax_ex, acc_3, gyro_3)\n",
    "            if f_triax is None:\n",
    "                continue\n",
    "            win_feats.append(f_triax)\n",
    "\n",
    "            # If we get here, the window is valid: add features + its label\n",
    "            file_feats.append(np.concatenate(win_feats))\n",
    "            file_labels.append(y[i])\n",
    "\n",
    "        if len(file_feats) == 0:\n",
    "            return None, None\n",
    "\n",
    "        return np.array(file_feats), np.array(file_labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing {filename}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# Instantiate Extractors (assuming classes are defined in kernel)\n",
    "print(\"Initializing extractors...\")\n",
    "eeg_ex = EEGFeatureExtractorLight(sfreq=250)\n",
    "ecg_ex = ECGFeatureExtractorLight(sfreq=250)\n",
    "emg_ex = EMGFeatureExtractorLight(sfreq=250)\n",
    "mov1d_ex = MOV1DFeatureExtractorLight(sfreq=25)\n",
    "mov_triax_ex = MOVTriaxFeatureExtractorLight(sfreq=25, vertical_axis=2)\n",
    "\n",
    "extractors_tuple = (eeg_ex, ecg_ex, emg_ex, mov1d_ex, mov_triax_ex)\n",
    "\n",
    "if manifest:\n",
    "    print(f\"Starting parallel extraction on {len(manifest)} files...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run Parallel\n",
    "    # Using n_jobs=4 to balance CPU/Memory\n",
    "    results = Parallel(n_jobs=4, verbose=5)( \n",
    "        delayed(process_file_manifest)(fname, idxs, extractors_tuple, DATA_DIR)\n",
    "        for fname, idxs in manifest.items()\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Extraction finished in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Aggregate Results\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    print(\"Aggregating results...\")\n",
    "    for X_f, y_f in results:\n",
    "        if X_f is not None and len(X_f) > 0:\n",
    "            X_list.append(X_f)\n",
    "            y_list.append(y_f)\n",
    "            \n",
    "    if X_list:\n",
    "        # Check consistency\n",
    "        shapes = [x.shape[1] for x in X_list]\n",
    "        if len(set(shapes)) > 1:\n",
    "            print(f\"Warning: Inconsistent feature shapes detected: {set(shapes)}\")\n",
    "            mode_shape = max(set(shapes), key=shapes.count)\n",
    "            print(f\"Filtering for mode shape: {mode_shape}\")\n",
    "            \n",
    "            X_list_clean = []\n",
    "            y_list_clean = []\n",
    "            for x, y in zip(X_list, y_list):\n",
    "                if x.shape[1] == mode_shape:\n",
    "                    X_list_clean.append(x)\n",
    "                    y_list_clean.append(y)\n",
    "            X_list = X_list_clean\n",
    "            y_list = y_list_clean\n",
    "\n",
    "        X_final = np.concatenate(X_list, axis=0)\n",
    "        y_final = np.concatenate(y_list, axis=0)\n",
    "        \n",
    "        print(f\"Final Feature Matrix Shape: {X_final.shape}\")\n",
    "        print(f\"Final Label Vector Shape: {y_final.shape}\")\n",
    "        \n",
    "        # Save\n",
    "        np.savez_compressed(OUTPUT_FILE, X=X_final, y=y_final)\n",
    "        print(f\"Saved optimized features to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No features extracted successfully.\")\n",
    "else:\n",
    "    print(\"Manifest is empty or not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa506da4",
   "metadata": {},
   "source": [
    "# SeizeIT2 Replicated Feature Engineering - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "from scipy.integrate import simpson\n",
    "import antropy as ant\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pywt\n",
    "\n",
    "class EEGFeatureExtractorSeizeIT2:\n",
    "    def __init__(self, sfreq=250):\n",
    "        self.sfreq = sfreq\n",
    "\n",
    "    # --- Helper: Compute Power Band Asymmetry ---\n",
    "\n",
    "\n",
    "    def extract_time_domain(self, x):\n",
    "        f = {}\n",
    "        # 1. Statistics\n",
    "        f['max'] = np.max(x)\n",
    "        f['min'] = np.min(x)\n",
    "        f['kurtosis'] = stats.kurtosis(x)\n",
    "        f['skew'] = stats.skew(x)\n",
    "            \n",
    "        # 4. Nonlinear / Entropy \n",
    "        f['sample_entropy'] = ant.sample_entropy(x)\n",
    "        f['spectral_entropy'] = ant.spectral_entropy(x, self.sfreq, method='welch', normalize=True)\n",
    "        f['shannon_entropy'] = stats.entropy(np.abs(x) + 1e-6)\n",
    "        \n",
    "        # 6. Energy/Power\n",
    "        f['rms'] = np.sqrt(np.mean(x**2))\n",
    "        \n",
    "        # 7. Patterns\n",
    "        f['zero_crossings'] = ant.num_zerocross(x)\n",
    "        \n",
    "        return f\n",
    "\n",
    "    def extract_frequency_domain(self, x):\n",
    "        f = {}\n",
    "        freqs, psd = signal.welch(x, self.sfreq, nperseg=min(len(x), 256))\n",
    "        psd_norm = psd / (np.sum(psd) + 1e-6)\n",
    "        \n",
    "        bands = {'delta': (1, 3), 'theta': (4, 8), 'alpha': (9, 13), 'beta': (14, 20)}\n",
    "        f['mean_power_delta'] = np.mean(psd[(freqs >= bands['delta'][0]) & (freqs < bands['delta'][1])])\n",
    "        f['mean_power_theta'] = np.mean(psd[(freqs >= bands['theta'][0]) & (freqs < bands['theta'][1])])\n",
    "        f['mean_power_alpha'] = np.mean(psd[(freqs >= bands['alpha'][0]) & (freqs < bands['alpha'][1])])\n",
    "        f['mean_power_beta'] = np.mean(psd[(freqs >= bands['beta'][0]) & (freqs < bands['beta'][1])])\n",
    "        \n",
    "        f['normalized_power_delta'] = np.sum(psd_norm[(freqs >= bands['delta'][0]) & (freqs < bands['delta'][1])])\n",
    "        f['normalized_power_theta'] = np.sum(psd_norm[(freqs >= bands['theta'][0]) & (freqs < bands['theta'][1])])\n",
    "        f['normalized_power_alpha'] = np.sum(psd_norm[(freqs >= bands['alpha'][0]) & (freqs < bands['alpha'][1])])\n",
    "        f['normalized_power_beta'] = np.sum(psd_norm[(freqs >= bands['beta'][0]) & (freqs < bands['beta'][1])])\n",
    "        \n",
    "        f['mean_power_hf_band'] = np.mean(psd[(freqs >= 40) and (freqs <= 80)])\n",
    "        f['normalized_power_hf_band'] = np.sum(psd_norm[(freqs >= 40) and (freqs <= 80)])\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def list_feature_names(self, x):\n",
    "        all_features = {}\n",
    "        for method in [\n",
    "            self.extract_time_domain,\n",
    "            self.extract_frequency_domain,\n",
    "        ]:\n",
    "            all_features.update(method(x))\n",
    "        \n",
    "        print(\"Total feature count:\", len(all_features))\n",
    "        print(\"Feature names:\")\n",
    "        for name in all_features:\n",
    "            print(name)\n",
    "\n",
    "        return list(all_features.keys())\n",
    "\n",
    "\n",
    "print(\"EEGFeatureExtractorSeizeIT2 updated with Time, Frequency, Time-Freq, and Decomposition domains.\")\n",
    "\n",
    "extractor = EEGFeatureExtractorSeizeIT2()\n",
    "dummy = np.random.randn(1000)  # 4 seconds at 250Hz\n",
    "\n",
    "_ = extractor.list_feature_names(dummy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
